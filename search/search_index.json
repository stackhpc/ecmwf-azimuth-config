{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Azimuth Operator Documentation This documentation describes how to manage deployments of Azimuth , including all the required dependencies. Azimuth is deployed using Ansible with playbooks from the azimuth-ops Ansible collection , driven by configuration derived from the azimuth-config reference configuration . The azimuth-config repository is designed to be forked for a specific site and is structured into multiple environments . This structure allows common configuration to be shared but overridden where required using composition of environments. Structure of an Azimuth deployment A fully-featured Azimuth deployment consists of many components - see Azimuth Architecture for details - and several of these components, including Zenith , AWX and Cluster API , require a Kubernetes cluster to run. However when you consider an Azimuth deployment as a whole, the only real dependency is an OpenStack cloud to target - we can create a Kubernetes cluster within an OpenStack project on the target cloud to host our Azimuth deployment. This is exactly what the playbooks in the azimuth-ops collection will do, when driven by a configuration derived from azimuth-config . There are two methods that azimuth-ops can use to deploy Azimuth and all of its dependencies: Onto a managed single-node K3S cluster in an OpenStack project. Onto a managed highly-available Kubernetes cluster in an OpenStack project. Option 1 is useful for development or demo deployments, but is not suitable for a production deployment. Option 2 is the recommended deployment mechanism for most deployments. In this mode, Terraform is used to provision a single-node K3S cluster that is configured as a Cluster API management cluster. Cluster API is then used to provision a highly-available Kubernetes cluster in the same OpenStack project onto which Azimuth is deployed. Warning Option 2 requires that Octavia is available on the target cloud to provide load-balancers for Azimuth components. Configuring and deploying Azimuth The rest of this documentation provides details on different aspects of building a production-ready Azimuth configuration and using it to deploy Azimuth. In the first instance it is recommended to follow the pages in order, implementing the steps in your Azimuth configuration. Once you have a working configuration, you can refer back to specific sections as needed.","title":"Home"},{"location":"#azimuth-operator-documentation","text":"This documentation describes how to manage deployments of Azimuth , including all the required dependencies. Azimuth is deployed using Ansible with playbooks from the azimuth-ops Ansible collection , driven by configuration derived from the azimuth-config reference configuration . The azimuth-config repository is designed to be forked for a specific site and is structured into multiple environments . This structure allows common configuration to be shared but overridden where required using composition of environments.","title":"Azimuth Operator Documentation"},{"location":"#structure-of-an-azimuth-deployment","text":"A fully-featured Azimuth deployment consists of many components - see Azimuth Architecture for details - and several of these components, including Zenith , AWX and Cluster API , require a Kubernetes cluster to run. However when you consider an Azimuth deployment as a whole, the only real dependency is an OpenStack cloud to target - we can create a Kubernetes cluster within an OpenStack project on the target cloud to host our Azimuth deployment. This is exactly what the playbooks in the azimuth-ops collection will do, when driven by a configuration derived from azimuth-config . There are two methods that azimuth-ops can use to deploy Azimuth and all of its dependencies: Onto a managed single-node K3S cluster in an OpenStack project. Onto a managed highly-available Kubernetes cluster in an OpenStack project. Option 1 is useful for development or demo deployments, but is not suitable for a production deployment. Option 2 is the recommended deployment mechanism for most deployments. In this mode, Terraform is used to provision a single-node K3S cluster that is configured as a Cluster API management cluster. Cluster API is then used to provision a highly-available Kubernetes cluster in the same OpenStack project onto which Azimuth is deployed. Warning Option 2 requires that Octavia is available on the target cloud to provide load-balancers for Azimuth components.","title":"Structure of an Azimuth deployment"},{"location":"#configuring-and-deploying-azimuth","text":"The rest of this documentation provides details on different aspects of building a production-ready Azimuth configuration and using it to deploy Azimuth. In the first instance it is recommended to follow the pages in order, implementing the steps in your Azimuth configuration. Once you have a working configuration, you can refer back to specific sections as needed.","title":"Configuring and deploying Azimuth"},{"location":"environments/","text":"Environments An Azimuth configuration repository is structured as multiple \"environments\" that can be composed. Some of these environments are \"concrete\", meaning that they provide enough information to make a deployment (e.g. development, staging, production), and some are \"mixin\" environments providing common configuration that can be incorporated into concrete environments using Ansible's support for multiple inventories . A concrete environment must be \"activated\" before any operations can be performed. Environments live in the environments directory of your configuration repository. A mixin environment contains only group_vars files and an empty hosts file (so that Ansible treats it as an inventory). A concrete environment must contain an ansible.cfg file defining the \"layering\" of inventories and a clouds.yaml file containing an OpenStack Application Credential for the project into which Azimuth will be deployed. Using mixin environments The following fragment demonstrates how to layer inventories in the ansible.cfg file for a highly-available (HA) deployment: ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory For a single node deployment, replace the ha environment with singlenode . Tip If the same variable is defined in multiple inventories, the right-most inventory takes precedence. Available mixin environments The following mixin environments are provided and maintained in this repository, and should be used as the basis for your concrete environments: base Contains the core configuration required to enable an environment and sets defaults. ha Contains overrides that are specific to an HA deployment. singlenode Contains overrides that are specific to a single-node deployment. By keeping the azimuth-config repository as an upstream of your site configuration repository, you can rebase onto or merge the latest configuration to pick up changes to these mixins. The azimuth-config repository contains an example of a concrete environment in environments/example that should be used as a basis for your own concrete environment(s). Depending how many concrete environments you have, you may wish to define mixin environments containing site-specific information that is common to several concrete environments, e.g. image and flavor IDs or the location of an ACME server. A typical layering of inventories might be: base -> singlenode -> site -> development base -> ha -> site -> staging base -> ha -> site -> production Linux environment variables azimuth-config environments are able to define Linux environment variables that are exported into the current shell when the environment is activated. This is accomplished by using statements of the form: env MY_VAR = \"some value\" The azimuth-config activate script exports environment variables defined in the following files: env and env.secret Contain environment variables that are common across all environments. environments/<env name>/env and environments/<env name>/env.secret Contain environment variables that are specific to the environment. In both cases, environment variables whose values should be kept private should be placed in the env.secret variant and should be encrypted .","title":"Environments"},{"location":"environments/#environments","text":"An Azimuth configuration repository is structured as multiple \"environments\" that can be composed. Some of these environments are \"concrete\", meaning that they provide enough information to make a deployment (e.g. development, staging, production), and some are \"mixin\" environments providing common configuration that can be incorporated into concrete environments using Ansible's support for multiple inventories . A concrete environment must be \"activated\" before any operations can be performed. Environments live in the environments directory of your configuration repository. A mixin environment contains only group_vars files and an empty hosts file (so that Ansible treats it as an inventory). A concrete environment must contain an ansible.cfg file defining the \"layering\" of inventories and a clouds.yaml file containing an OpenStack Application Credential for the project into which Azimuth will be deployed.","title":"Environments"},{"location":"environments/#using-mixin-environments","text":"The following fragment demonstrates how to layer inventories in the ansible.cfg file for a highly-available (HA) deployment: ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory For a single node deployment, replace the ha environment with singlenode . Tip If the same variable is defined in multiple inventories, the right-most inventory takes precedence.","title":"Using mixin environments"},{"location":"environments/#available-mixin-environments","text":"The following mixin environments are provided and maintained in this repository, and should be used as the basis for your concrete environments: base Contains the core configuration required to enable an environment and sets defaults. ha Contains overrides that are specific to an HA deployment. singlenode Contains overrides that are specific to a single-node deployment. By keeping the azimuth-config repository as an upstream of your site configuration repository, you can rebase onto or merge the latest configuration to pick up changes to these mixins. The azimuth-config repository contains an example of a concrete environment in environments/example that should be used as a basis for your own concrete environment(s). Depending how many concrete environments you have, you may wish to define mixin environments containing site-specific information that is common to several concrete environments, e.g. image and flavor IDs or the location of an ACME server. A typical layering of inventories might be: base -> singlenode -> site -> development base -> ha -> site -> staging base -> ha -> site -> production","title":"Available mixin environments"},{"location":"environments/#linux-environment-variables","text":"azimuth-config environments are able to define Linux environment variables that are exported into the current shell when the environment is activated. This is accomplished by using statements of the form: env MY_VAR = \"some value\" The azimuth-config activate script exports environment variables defined in the following files: env and env.secret Contain environment variables that are common across all environments. environments/<env name>/env and environments/<env name>/env.secret Contain environment variables that are specific to the environment. In both cases, environment variables whose values should be kept private should be placed in the env.secret variant and should be encrypted .","title":"Linux environment variables"},{"location":"try/","text":"Try Azimuth If you have access to a project on an OpenStack cloud, you can try Azimuth! The azimuth-config repository contains a special environment called demo that will provision a short-lived Azimuth deployment for demonstration purposes only . This environment attempts to infer all required configuration from the target OpenStack cloud - if this process is unsuccessful, an error will be produced. Danger Inferring configuration in the way that the demo environment does is not recommended as it is not guaranteed to produce the same result each time. For production deployments it is better to be explicit. Deploying a demo instance The Azimuth deployment requires a clouds.yaml to run. Ideally, this should be an Application Credential . Once you have a clouds.yaml , run the following to deploy the Azimuth demo environment: # Set OpenStack configuration variables export OS_CLOUD = openstack export OS_CLIENT_CONFIG_FILE = /path/to/clouds.yaml # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config cd azimuth-config # Set up the virtual environment ./bin/ensure-venv # Activate the demo environment source ./bin/activate demo # Install Ansible dependencies ansible-galaxy install -f -r requirements.yml # Deploy Azimuth ansible-playbook stackhpc.azimuth_ops.provision The URL for the Azimuth UI is printed at the end of the playbook run. The credentials you use to authenticate with Azimuth are the same as you would use with the underlying OpenStack cloud. Warning Azimuth is deployed using Ansible, which does not support Windows as a controller . Azimuth deployment has been tested on Linux and macOS. Limitations The demo deployment has a number of limitations in order to give it the best chance of running on any given cloud: It uses the single node deployment method . Community images are uploaded as private images, so Azimuth will only be able to provision Kubernetes clusters and Cluster-as-a-Service appliances in the same project as it is deployed in. sslip.io is used to provide DNS. This avoids the need for a DNS entry to be provisioned in advance. TLS is disabled for ingress , allowing the Azimuth to work even when the deployment is not reachable from the internet ( outbound internet connectivity is still required). Verification of SSL certificates for the OpenStack API is disabled, allowing Azimuth to work even when the target cloud uses a custom CA. The deployment secrets are not secret , as they are stored in plain text in the azimuth-config repository on GitHub.","title":"Try Azimuth"},{"location":"try/#try-azimuth","text":"If you have access to a project on an OpenStack cloud, you can try Azimuth! The azimuth-config repository contains a special environment called demo that will provision a short-lived Azimuth deployment for demonstration purposes only . This environment attempts to infer all required configuration from the target OpenStack cloud - if this process is unsuccessful, an error will be produced. Danger Inferring configuration in the way that the demo environment does is not recommended as it is not guaranteed to produce the same result each time. For production deployments it is better to be explicit.","title":"Try Azimuth"},{"location":"try/#deploying-a-demo-instance","text":"The Azimuth deployment requires a clouds.yaml to run. Ideally, this should be an Application Credential . Once you have a clouds.yaml , run the following to deploy the Azimuth demo environment: # Set OpenStack configuration variables export OS_CLOUD = openstack export OS_CLIENT_CONFIG_FILE = /path/to/clouds.yaml # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config cd azimuth-config # Set up the virtual environment ./bin/ensure-venv # Activate the demo environment source ./bin/activate demo # Install Ansible dependencies ansible-galaxy install -f -r requirements.yml # Deploy Azimuth ansible-playbook stackhpc.azimuth_ops.provision The URL for the Azimuth UI is printed at the end of the playbook run. The credentials you use to authenticate with Azimuth are the same as you would use with the underlying OpenStack cloud. Warning Azimuth is deployed using Ansible, which does not support Windows as a controller . Azimuth deployment has been tested on Linux and macOS.","title":"Deploying a demo instance"},{"location":"try/#limitations","text":"The demo deployment has a number of limitations in order to give it the best chance of running on any given cloud: It uses the single node deployment method . Community images are uploaded as private images, so Azimuth will only be able to provision Kubernetes clusters and Cluster-as-a-Service appliances in the same project as it is deployed in. sslip.io is used to provide DNS. This avoids the need for a DNS entry to be provisioned in advance. TLS is disabled for ingress , allowing the Azimuth to work even when the deployment is not reachable from the internet ( outbound internet connectivity is still required). Verification of SSL certificates for the OpenStack API is disabled, allowing Azimuth to work even when the target cloud uses a custom CA. The deployment secrets are not secret , as they are stored in plain text in the azimuth-config repository on GitHub.","title":"Limitations"},{"location":"configuration/","text":"Customising the Azimuth configuration The roles in the azimuth-ops collection support a vast array of variables to customise an Azimuth deployment. However azimuth-ops endeavours to pick sensible defaults where appropriate, so this documentation focuses on configuration that is required or commonly changed. For more advanced cases, the role defaults files are extensively documented and can be consulted directly. Note Make sure you are familiar with the Azimuth and Zenith architectures before continuing. It is assumed that you have already followed the steps in Setting up a configuration repository , and so have an environment for your site that is ready to be configured.","title":"Customising the Azimuth configuration"},{"location":"configuration/#customising-the-azimuth-configuration","text":"The roles in the azimuth-ops collection support a vast array of variables to customise an Azimuth deployment. However azimuth-ops endeavours to pick sensible defaults where appropriate, so this documentation focuses on configuration that is required or commonly changed. For more advanced cases, the role defaults files are extensively documented and can be consulted directly. Note Make sure you are familiar with the Azimuth and Zenith architectures before continuing. It is assumed that you have already followed the steps in Setting up a configuration repository , and so have an environment for your site that is ready to be configured.","title":"Customising the Azimuth configuration"},{"location":"configuration/01-prerequisites/","text":"Prerequisites In order to deploy Azimuth, a small number of prerequisites must be fulfilled. OpenStack cloud Firstly, there must be an OpenStack cloud onto which you will deploy Azimuth and which the Azimuth installation will target. The target cloud must support IPv4 and have an \"external\" network that floating IPs can be allocated on. The target cloud must also support load-balancers via Octavia if you want to use the highly-available deployment mode or support Kubernetes Ingress or LoadBalancer services on tenant Kubernetes clusters. OpenStack project quotas There must be an OpenStack project into which Azimuth will be deployed, with appropriate quotas. In particular, for a high-availability deployment the project should be permitted to use three floating IPs - one for accessing the \"management node\", one for the Kubernetes Ingress controller and one for the Zenith SSHD server. Application Credential You should create an Application Credential for the project and save the resulting clouds.yaml as ./environments/<name>/clouds.yaml . Wildcard DNS As discussed in the Azimuth Architecture document, Azimuth and Zenith expect to be given control of a entire subdomain, e.g. *.apps.example.org , where Azimuth will be exposed as portal.apps.example.org and Zenith services will have domains of the form <subdomain>.apps.example.org . azimuth-ops does not manage DNS records, so you must allocate a floating IP to the project and ensure that a wildcard DNS entry exists for your chosen subdomain that points to the allocated IP. This IP will be used for the Kubernetes Ingress controller.","title":"Prerequisites"},{"location":"configuration/01-prerequisites/#prerequisites","text":"In order to deploy Azimuth, a small number of prerequisites must be fulfilled.","title":"Prerequisites"},{"location":"configuration/01-prerequisites/#openstack-cloud","text":"Firstly, there must be an OpenStack cloud onto which you will deploy Azimuth and which the Azimuth installation will target. The target cloud must support IPv4 and have an \"external\" network that floating IPs can be allocated on. The target cloud must also support load-balancers via Octavia if you want to use the highly-available deployment mode or support Kubernetes Ingress or LoadBalancer services on tenant Kubernetes clusters.","title":"OpenStack cloud"},{"location":"configuration/01-prerequisites/#openstack-project-quotas","text":"There must be an OpenStack project into which Azimuth will be deployed, with appropriate quotas. In particular, for a high-availability deployment the project should be permitted to use three floating IPs - one for accessing the \"management node\", one for the Kubernetes Ingress controller and one for the Zenith SSHD server.","title":"OpenStack project quotas"},{"location":"configuration/01-prerequisites/#application-credential","text":"You should create an Application Credential for the project and save the resulting clouds.yaml as ./environments/<name>/clouds.yaml .","title":"Application Credential"},{"location":"configuration/01-prerequisites/#wildcard-dns","text":"As discussed in the Azimuth Architecture document, Azimuth and Zenith expect to be given control of a entire subdomain, e.g. *.apps.example.org , where Azimuth will be exposed as portal.apps.example.org and Zenith services will have domains of the form <subdomain>.apps.example.org . azimuth-ops does not manage DNS records, so you must allocate a floating IP to the project and ensure that a wildcard DNS entry exists for your chosen subdomain that points to the allocated IP. This IP will be used for the Kubernetes Ingress controller.","title":"Wildcard DNS"},{"location":"configuration/02-deployment-method/","text":"Deployment method azimuth-ops supports two deployment methods - singlenode and ha . Networking automation azimuth-ops will create an internal network, onto which all nodes for the deployment will be placed. It will also create a router connecting the internal network to the external network where floating IPs are allocated. Single node In this deployment method, a single node is provisioned with Terraform and configured as a K3S cluster. The full Azimuth stack is then deployed onto this cluster. Warning This deployment method is only suitable for development or demonstration. To use the single node deployment method, use the singlenode environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../singlenode/inventory,./inventory The following variables must be set to define the properties of the K3S node: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The floating IP to which to wildcard DNS entry has been assigned infra_fixed_floatingip : \"<pre-allocated floating ip>\" # The ID of the flavor to use for the K3S node # A flavor with at least 4 CPUs and 16GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The size of the volume to use for K3S cluster data infra_data_volume_size : 100 Highly-available (HA) For the HA deployment method, Terraform is also used to provision a single node that is configured as a K3S cluster. However rather than hosting the Azimuth components, as in the single node case, this K3S cluster is only configured as a Cluster API management cluster . Cluster API on the K3S cluster is then used to manage a HA cluster, in the same project and on the same network. The Azimuth stack is then deployed onto this cluster. To use the HA deployment method, use the ha environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory The following variables must be set to define the properties of the K3S node and the Cluster API managed nodes: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The ID of the flavor to use for the K3S node # A flavor with at least 2 CPUs and 8GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The name of the flavor to use for control plane nodes # A flavor with at least 2 CPUs, 8GB RAM and 100GB root disk is recommended capi_cluster_control_plane_flavor : \"<flavor name>\" # The name of the flavor to use for worker nodes # A flavor with at least 4 CPUs, 16GB RAM and 100GB root disk is recommended capi_cluster_worker_flavor : \"<flavor name>\" # The number of worker nodes capi_cluster_worker_count : 3 # The floating IP to which to wildcard DNS entry has been assigned capi_cluster_addons_ingress_load_balancer_ip : \"<pre-allocated floating ip>\" Availability Zones for Kubernetes nodes Also applies to tenant Kubernetes clusters The concepts in this section apply to any Kubernetes clusters created using Cluster API, i.e. the HA cluster in a HA deployment and tenant clusters. The variable names differ slightly for the two cases. By default, an Azimuth installation assumes that there is a single availability zone (AZ) called nova - this is the default set up and common for small-to-medium sized clouds. If this is not the case for your target cloud, you may need to set additional variables specifying the AZs to use, both for the HA cluster and for tenant Kubernetes clusters. The default behaviour when scheduling Kubernetes nodes using Cluster API is: All available AZs are considered for control plane nodes, and Cluster API will attempt to spread the nodes across multiple AZs, if available. Worker nodes are scheduled into the nova AZ explicitly. If this AZ does not exist, scheduling will fail. If this default behaviour does not work for your target cloud, the following options are available. Note Cluster API refers to \"failure domains\" which, in the OpenStack provider, correspond to availability zones (AZs). Use specific availability zones To specify the availability zones for Kubernetes nodes, the following variables can be used: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # A list of failure domains that should be considered for control plane nodes capi_cluster_control_plane_failure_domains : [ az1 , az2 ] # The failure domain for worker nodes capi_cluster_worker_failure_domain : az1 #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_failure_domains : [ az1 , az2 ] azimuth_capi_operator_capi_helm_worker_failure_domain : az1 Ignore availability zones It is possible to configure Cluster API clusters in such a way that AZs are not specified at all for Kubernetes nodes. This allows other placement constraints such as flavor traits and host aggregate to be used, and a suitable AZ to be selected by OpenStack. environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # Indicate that the failure domain should be omitted for control plane nodes capi_cluster_control_plane_omit_failure_domain : true # Specify no failure domain for worker nodes capi_cluster_worker_failure_domain : null #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain : true azimuth_capi_operator_capi_helm_worker_failure_domain : null","title":"Deployment method"},{"location":"configuration/02-deployment-method/#deployment-method","text":"azimuth-ops supports two deployment methods - singlenode and ha . Networking automation azimuth-ops will create an internal network, onto which all nodes for the deployment will be placed. It will also create a router connecting the internal network to the external network where floating IPs are allocated.","title":"Deployment method"},{"location":"configuration/02-deployment-method/#single-node","text":"In this deployment method, a single node is provisioned with Terraform and configured as a K3S cluster. The full Azimuth stack is then deployed onto this cluster. Warning This deployment method is only suitable for development or demonstration. To use the single node deployment method, use the singlenode environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../singlenode/inventory,./inventory The following variables must be set to define the properties of the K3S node: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The floating IP to which to wildcard DNS entry has been assigned infra_fixed_floatingip : \"<pre-allocated floating ip>\" # The ID of the flavor to use for the K3S node # A flavor with at least 4 CPUs and 16GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The size of the volume to use for K3S cluster data infra_data_volume_size : 100","title":"Single node"},{"location":"configuration/02-deployment-method/#highly-available-ha","text":"For the HA deployment method, Terraform is also used to provision a single node that is configured as a K3S cluster. However rather than hosting the Azimuth components, as in the single node case, this K3S cluster is only configured as a Cluster API management cluster . Cluster API on the K3S cluster is then used to manage a HA cluster, in the same project and on the same network. The Azimuth stack is then deployed onto this cluster. To use the HA deployment method, use the ha environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory The following variables must be set to define the properties of the K3S node and the Cluster API managed nodes: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The ID of the flavor to use for the K3S node # A flavor with at least 2 CPUs and 8GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The name of the flavor to use for control plane nodes # A flavor with at least 2 CPUs, 8GB RAM and 100GB root disk is recommended capi_cluster_control_plane_flavor : \"<flavor name>\" # The name of the flavor to use for worker nodes # A flavor with at least 4 CPUs, 16GB RAM and 100GB root disk is recommended capi_cluster_worker_flavor : \"<flavor name>\" # The number of worker nodes capi_cluster_worker_count : 3 # The floating IP to which to wildcard DNS entry has been assigned capi_cluster_addons_ingress_load_balancer_ip : \"<pre-allocated floating ip>\"","title":"Highly-available (HA)"},{"location":"configuration/02-deployment-method/#availability-zones-for-kubernetes-nodes","text":"Also applies to tenant Kubernetes clusters The concepts in this section apply to any Kubernetes clusters created using Cluster API, i.e. the HA cluster in a HA deployment and tenant clusters. The variable names differ slightly for the two cases. By default, an Azimuth installation assumes that there is a single availability zone (AZ) called nova - this is the default set up and common for small-to-medium sized clouds. If this is not the case for your target cloud, you may need to set additional variables specifying the AZs to use, both for the HA cluster and for tenant Kubernetes clusters. The default behaviour when scheduling Kubernetes nodes using Cluster API is: All available AZs are considered for control plane nodes, and Cluster API will attempt to spread the nodes across multiple AZs, if available. Worker nodes are scheduled into the nova AZ explicitly. If this AZ does not exist, scheduling will fail. If this default behaviour does not work for your target cloud, the following options are available. Note Cluster API refers to \"failure domains\" which, in the OpenStack provider, correspond to availability zones (AZs).","title":"Availability Zones for Kubernetes nodes"},{"location":"configuration/02-deployment-method/#use-specific-availability-zones","text":"To specify the availability zones for Kubernetes nodes, the following variables can be used: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # A list of failure domains that should be considered for control plane nodes capi_cluster_control_plane_failure_domains : [ az1 , az2 ] # The failure domain for worker nodes capi_cluster_worker_failure_domain : az1 #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_failure_domains : [ az1 , az2 ] azimuth_capi_operator_capi_helm_worker_failure_domain : az1","title":"Use specific availability zones"},{"location":"configuration/02-deployment-method/#ignore-availability-zones","text":"It is possible to configure Cluster API clusters in such a way that AZs are not specified at all for Kubernetes nodes. This allows other placement constraints such as flavor traits and host aggregate to be used, and a suitable AZ to be selected by OpenStack. environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # Indicate that the failure domain should be omitted for control plane nodes capi_cluster_control_plane_omit_failure_domain : true # Specify no failure domain for worker nodes capi_cluster_worker_failure_domain : null #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain : true azimuth_capi_operator_capi_helm_worker_failure_domain : null","title":"Ignore availability zones"},{"location":"configuration/03-target-cloud/","text":"Target cloud The main piece of site-specific configuration required by Azimuth is the connection information for the target OpenStack cloud. Azimuth uses the Keystone Service Catalog to discover the endpoints for OpenStack services, so only needs to be told where to find the Keystone v3 endpoint: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_auth_url : https://openstack.example-cloud.org:5000/v3 Warning Make sure to include the trailing /v3 , otherwise authentication will fail. Azimuth does not currently have support for specifying a custom CA for verifying TLS. If the target cloud uses a TLS certificate that is not verifiable using the operating-system default trustroots, TLS verification must be disabled: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_verify_ssl : false If you use a domain other than default , you will also need to tell Azimuth the name of the domain to use when authenticating: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_domain : my-domain Cloud name Azimuth presents the name of the current cloud in various places in the interface. To configure this, set the following variables: environments/my-site/inventory/group_vars/all/variables.yml # The machine-readable cloud name azimuth_current_cloud_name : my-cloud # The human-readable name for the cloud azimuth_current_cloud_label : My Cloud Federated authentication By default the password authenticator is enabled, which accepts a username and password and swaps them for an OpenStack token. This requires no additional configuration. If the target cloud consumes identities from an external provider via Keystone federation , then Azimuth can be configured to obtain an OpenStack token from Keystone using the same flow as Horizon. To enable this, additional configuration is required for both Azimuth and Keystone on the target cloud. First, the Keystone configuration of the target cloud must be modified to add Azimuth as a trusted dashboard , otherwise it will be unable to retrieve a token via the federated flow. When configuring Azimuth as a trusted dashboard, you must specify the URL that will receive token data - for an Azimuth deployment, this URL is https://[portal domain]/auth/federated/complete/ , where the portal domain depends on the ingress configuration as described elsewhere in this documentation. In your Azimuth configuration, enable the federated authenticator and tell it the provider and protocol to use: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_federated_enabled : yes azimuth_authenticator_federated_provider : \"<provider>\" azimuth_authenticator_federated_protocol : \"<protocol>\" This will result in Azimuth using URLs of the following form for the federated authentication flow: <auth url>/auth/OS-FEDERATION/identity_providers/<provider>/protocols/<protocol>/websso The provider and protocol will depend on the Keystone configuration of the target OpenStack cloud. To also disable the password authenticator - so that federation is the only supported login - set the following variable: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_enabled : no To change the human-readable names for the authenticators, which are presented in the authentication method selection form, use the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_label : \"Username + Password\" azimuth_authenticator_federated_label : \"Federated\" Networking configuration Azimuth uses Neutron resource tags to discover the networks it should use, and the tags it looks for are portal-internal and portal-external for the internal and external networks respectively. These tags must be applied by the cloud operator. If it cannot find a tagged internal network, the default behaviour is for Azimuth to create an internal network to use (and the corresponding router to attach it to the external network). The discovery and auto-creation process is described in detail in Network discovery and auto-creation . To disable the auto-creation of internal networks, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_create_internal_net : false The CIDR of the auto-created subnet can also be changed, although it is the same for every project. For example, you may need to do this if the default CIDR conflicts with resources elsewhere on your network that machines provisioned by Azimuth need to access: environments/my-site/inventory/group_vars/all/variables.yml # Defaults to 192.168.3.0/24 azimuth_openstack_internal_net_cidr : 10.0.3.0/24","title":"Target cloud"},{"location":"configuration/03-target-cloud/#target-cloud","text":"The main piece of site-specific configuration required by Azimuth is the connection information for the target OpenStack cloud. Azimuth uses the Keystone Service Catalog to discover the endpoints for OpenStack services, so only needs to be told where to find the Keystone v3 endpoint: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_auth_url : https://openstack.example-cloud.org:5000/v3 Warning Make sure to include the trailing /v3 , otherwise authentication will fail. Azimuth does not currently have support for specifying a custom CA for verifying TLS. If the target cloud uses a TLS certificate that is not verifiable using the operating-system default trustroots, TLS verification must be disabled: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_verify_ssl : false If you use a domain other than default , you will also need to tell Azimuth the name of the domain to use when authenticating: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_domain : my-domain","title":"Target cloud"},{"location":"configuration/03-target-cloud/#cloud-name","text":"Azimuth presents the name of the current cloud in various places in the interface. To configure this, set the following variables: environments/my-site/inventory/group_vars/all/variables.yml # The machine-readable cloud name azimuth_current_cloud_name : my-cloud # The human-readable name for the cloud azimuth_current_cloud_label : My Cloud","title":"Cloud name"},{"location":"configuration/03-target-cloud/#federated-authentication","text":"By default the password authenticator is enabled, which accepts a username and password and swaps them for an OpenStack token. This requires no additional configuration. If the target cloud consumes identities from an external provider via Keystone federation , then Azimuth can be configured to obtain an OpenStack token from Keystone using the same flow as Horizon. To enable this, additional configuration is required for both Azimuth and Keystone on the target cloud. First, the Keystone configuration of the target cloud must be modified to add Azimuth as a trusted dashboard , otherwise it will be unable to retrieve a token via the federated flow. When configuring Azimuth as a trusted dashboard, you must specify the URL that will receive token data - for an Azimuth deployment, this URL is https://[portal domain]/auth/federated/complete/ , where the portal domain depends on the ingress configuration as described elsewhere in this documentation. In your Azimuth configuration, enable the federated authenticator and tell it the provider and protocol to use: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_federated_enabled : yes azimuth_authenticator_federated_provider : \"<provider>\" azimuth_authenticator_federated_protocol : \"<protocol>\" This will result in Azimuth using URLs of the following form for the federated authentication flow: <auth url>/auth/OS-FEDERATION/identity_providers/<provider>/protocols/<protocol>/websso The provider and protocol will depend on the Keystone configuration of the target OpenStack cloud. To also disable the password authenticator - so that federation is the only supported login - set the following variable: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_enabled : no To change the human-readable names for the authenticators, which are presented in the authentication method selection form, use the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_label : \"Username + Password\" azimuth_authenticator_federated_label : \"Federated\"","title":"Federated authentication"},{"location":"configuration/03-target-cloud/#networking-configuration","text":"Azimuth uses Neutron resource tags to discover the networks it should use, and the tags it looks for are portal-internal and portal-external for the internal and external networks respectively. These tags must be applied by the cloud operator. If it cannot find a tagged internal network, the default behaviour is for Azimuth to create an internal network to use (and the corresponding router to attach it to the external network). The discovery and auto-creation process is described in detail in Network discovery and auto-creation . To disable the auto-creation of internal networks, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_create_internal_net : false The CIDR of the auto-created subnet can also be changed, although it is the same for every project. For example, you may need to do this if the default CIDR conflicts with resources elsewhere on your network that machines provisioned by Azimuth need to access: environments/my-site/inventory/group_vars/all/variables.yml # Defaults to 192.168.3.0/24 azimuth_openstack_internal_net_cidr : 10.0.3.0/24","title":"Networking configuration"},{"location":"configuration/04-secret-key/","text":"Secret key Azimuth requires a secret key that is used primarily for signing cookies: environments/my-site/inventory/group_vars/all/secrets.yml azimuth_secret_key : \"<some secret key>\" Tip This key should be a long, random string - at least 32 bytes (256 bits) is recommended. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Secret key"},{"location":"configuration/04-secret-key/#secret-key","text":"Azimuth requires a secret key that is used primarily for signing cookies: environments/my-site/inventory/group_vars/all/secrets.yml azimuth_secret_key : \"<some secret key>\" Tip This key should be a long, random string - at least 32 bytes (256 bits) is recommended. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Secret key"},{"location":"configuration/05-ingress/","text":"Ingress As mentioned in the prerequisites, Azimuth and Zenith expect to be given control of entire subdomain, e.g. *.apps.example.org and this domain must be assigned to a pre-allocated floating IP using a wildcard DNS entry. To tell azimuth-ops what domain it should use, simply set the following variable: environments/my-site/inventory/group_vars/all/variables.yml ingress_base_domain : apps.example.org This will result in azimuth-ops using portal.apps.example.org for Azimuth and registrar.apps.example.org for the Zenith registrar. If Harbor is enabled, registry.apps.example.org will be used for the Harbor registry. Zenith will use domains of the form <random subdomain>.apps.example.org for its services. Transport Layer Security (TLS) TLS for Azimuth can be configured in two ways: Pre-existing wildcard certificate If you have a pre-existing wildcard TLS certificate issued for the ingress base domain, you can use this to provide TLS for Azimuth and Zenith services. Tip At the time of writing this is the recommended mechanism for production deployments, despite the lack of automation, for two reasons: It is not affected by rate limits Zenith services become available faster as it avoids the overhead of obtaining a certificate per service Danger It is your responsibility to check for the expiry of the certificate and renew it. Consider using an external service to notify you when the certificate is approaching its expiry date. To configure a pre-existing wildcard certificate for ingress, just create the following files in your environment: environments/my-site/tls/tls.crt Must contain the full certificate chain , with the most specific certificate at the top (e.g. the wildcard certificate), any intermediate certificates and the root CA at the bottom. environments/my-site/tls/tls.key The corresponding private key for the wildcard certificate. Danger The TLS key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted . When these files exist in an environment, azimuth-ops will automatically pick them up and use them. Info In the future, support for a wildcard certificate managed by cert-manager may be implemented. However this will require the use of a supported DNS provider in order to fulfil the DNS01 challenge (wildcard certificates cannot be issued for an HTTP01 challenge). Automated with cert-manager azimuth-ops is able to configure Ingress resources for Azimuth and Zenith services so that their TLS certificates are managed automatically using cert-manager . In this configuration, a certificate is issued for each separate subdomain by an ACME provider using the HTTP-01 challenge type . By default, cert-manager is enabled and this mechanism will be used to issue TLS certificates for Azimuth and Zenith services with no further configuration. The default ACME service is Let's Encrypt , which issues certificates that are trusted by all major operating systems and browsers. Let's Encrypt rate limits Let's Encrypt imposes rate limits to ensure fair usage. At the time of writing, the number of new certificates that can be issued is 50 per week per registed domain . The \"registered domain\" is the part of the domain that is purchased from the registrar so, for an Azimuth deployment with an ingress base domain of *.apps.example.org , the Let's Encrypt rate limit is imposed on example.org . If there are a large number of Zenith services and the rate limit is reached, cert-manager will not be able to obtain a certificate and Azimuth and Zenith services will never become available.","title":"Ingress"},{"location":"configuration/05-ingress/#ingress","text":"As mentioned in the prerequisites, Azimuth and Zenith expect to be given control of entire subdomain, e.g. *.apps.example.org and this domain must be assigned to a pre-allocated floating IP using a wildcard DNS entry. To tell azimuth-ops what domain it should use, simply set the following variable: environments/my-site/inventory/group_vars/all/variables.yml ingress_base_domain : apps.example.org This will result in azimuth-ops using portal.apps.example.org for Azimuth and registrar.apps.example.org for the Zenith registrar. If Harbor is enabled, registry.apps.example.org will be used for the Harbor registry. Zenith will use domains of the form <random subdomain>.apps.example.org for its services.","title":"Ingress"},{"location":"configuration/05-ingress/#transport-layer-security-tls","text":"TLS for Azimuth can be configured in two ways:","title":"Transport Layer Security (TLS)"},{"location":"configuration/05-ingress/#pre-existing-wildcard-certificate","text":"If you have a pre-existing wildcard TLS certificate issued for the ingress base domain, you can use this to provide TLS for Azimuth and Zenith services. Tip At the time of writing this is the recommended mechanism for production deployments, despite the lack of automation, for two reasons: It is not affected by rate limits Zenith services become available faster as it avoids the overhead of obtaining a certificate per service Danger It is your responsibility to check for the expiry of the certificate and renew it. Consider using an external service to notify you when the certificate is approaching its expiry date. To configure a pre-existing wildcard certificate for ingress, just create the following files in your environment: environments/my-site/tls/tls.crt Must contain the full certificate chain , with the most specific certificate at the top (e.g. the wildcard certificate), any intermediate certificates and the root CA at the bottom. environments/my-site/tls/tls.key The corresponding private key for the wildcard certificate. Danger The TLS key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted . When these files exist in an environment, azimuth-ops will automatically pick them up and use them. Info In the future, support for a wildcard certificate managed by cert-manager may be implemented. However this will require the use of a supported DNS provider in order to fulfil the DNS01 challenge (wildcard certificates cannot be issued for an HTTP01 challenge).","title":"Pre-existing wildcard certificate"},{"location":"configuration/05-ingress/#automated-with-cert-manager","text":"azimuth-ops is able to configure Ingress resources for Azimuth and Zenith services so that their TLS certificates are managed automatically using cert-manager . In this configuration, a certificate is issued for each separate subdomain by an ACME provider using the HTTP-01 challenge type . By default, cert-manager is enabled and this mechanism will be used to issue TLS certificates for Azimuth and Zenith services with no further configuration. The default ACME service is Let's Encrypt , which issues certificates that are trusted by all major operating systems and browsers. Let's Encrypt rate limits Let's Encrypt imposes rate limits to ensure fair usage. At the time of writing, the number of new certificates that can be issued is 50 per week per registed domain . The \"registered domain\" is the part of the domain that is purchased from the registrar so, for an Azimuth deployment with an ingress base domain of *.apps.example.org , the Let's Encrypt rate limit is imposed on example.org . If there are a large number of Zenith services and the rate limit is reached, cert-manager will not be able to obtain a certificate and Azimuth and Zenith services will never become available.","title":"Automated with cert-manager"},{"location":"configuration/06-platform-identity/","text":"Platform Identity In Azimuth, there are two kinds of users: Platform Users who are able to access one or more platforms deployed by Azimuth. Platform Admins who are able to sign in to Azimuth, manage the deployed platforms in a tenancy and administer access to those platforms. Only Platform Admins need to have an OpenStack account. Each Azimuth tenancy has an associated Identity Realm that provides the Platform Users for that tenancy and is administered by the Platform Admins for the tenancy. This allow access to platforms to be granted for users that do not have an OpenStack account. This separation of Platform Users from OpenStack accounts opens up a wide range of use cases that are not possible when platforms can only be accessed by users with OpenStack accounts. Example use case Imagine that a project with resource on an OpenStack cloud wishes to host an open workshop using Jupyter notebooks for teaching. In this case, they definitely don't want to grant every workshop attendee access to OpenStack, as these users may not be trusted. Using Azimuth, a trusted project member can deploy a JupyterHub in Azimuth (i.e. as a Platform Admin) and create users in their Identity Realm for the workshop attendees. These users can be granted access to JupyterHub for the duration of the workshop, and at the end of the workshop their access can be revoked. Use of Keycloak In order to accomplish this, every Azimuth installation includes an instance of the Keycloak open-source identity management platform. Each Azimuth tenancy (i.e. OpenStack project) has an associated realm in this Keycloak instance that is used to manage access to platforms deployed in that tenancy. The realm provides authentication and authorization for the Zenith services associated with the platforms in the tenancy using OpenID Connect (OIDC) . Each realm is created with two groups - admins and platform-users . Users who are in the platform-users group for a realm are granted access to all platforms deployed in the corresponding Azimuth tenancy. Users who are in the admins group are granted admin status for the realm, meaning they can perform actions in the Keycloak admin console such as: Managing users Assigning users to groups Configuring authentication policies , e.g. password requirements, multi-factor authentication Integrating external identity providers The Keycloak realms created by Azimuth are configured with Azimuth as an identity provider, so that Platform Admins who belong to a tenancy in Azimuth can sign in to the corresponding realm using the Azimuth identity provider. Platform Admins who sign in to a realm via Azimuth are automatically placed in the admins and platform-users groups described above. Note This means that all users that can access the Azimuth tenancy by signing in with an OpenStack account (i.e. Platform Admins) automatically get access to all the platforms in the tenancy. This matches the behaviour of the previous shared cookie authentication, so existing Azimuth users will still be able to use their existing workflows. However the new approach uses an industry standard protocol designed specifically for federated identity in an untrusted environment, and as such eliminates a potential security flaw in the previous shared cookie approach that could leak unscoped OpenStack tokens to a malicious application exposed via Zenith. When a platform is deployed in an Azimuth tenancy a group is created in the corresponding identity realm, and access to the platform is controlled by membership of this group. Each Zenith service for the platform has a child group under the platform group that can be used to grant access to a single service within a platform. For example, the standard Linux Workstation platform (see Cluster-as-a-Service (CaaS) ) exposes two Zenith services - \"Web Console\" and \"Monitoring\". If an instance of this platform is deployed with the name my-workstation , then access to the \"Web Console\" service can be granted by either: Adding the user to the platform-users group. This will also grant the user access to all other platforms in the tenancy. Adding the user to the caas-my-workstation group. This will grant the user access to the \"Web Console\" and \"Monitoring\" services. Adding the user to the caas-my-workstation/webconsole group. This will grant the user access to the \"Web Console\" service only. Keycloak admin password The only required configuration for platform identity is to set the admin password for Keycloak: environments/my-site/inventory/group_vars/all/secrets.yml keycloak_admin_password : \"<secure password>\" Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted .","title":"Platform Identity"},{"location":"configuration/06-platform-identity/#platform-identity","text":"In Azimuth, there are two kinds of users: Platform Users who are able to access one or more platforms deployed by Azimuth. Platform Admins who are able to sign in to Azimuth, manage the deployed platforms in a tenancy and administer access to those platforms. Only Platform Admins need to have an OpenStack account. Each Azimuth tenancy has an associated Identity Realm that provides the Platform Users for that tenancy and is administered by the Platform Admins for the tenancy. This allow access to platforms to be granted for users that do not have an OpenStack account. This separation of Platform Users from OpenStack accounts opens up a wide range of use cases that are not possible when platforms can only be accessed by users with OpenStack accounts.","title":"Platform Identity"},{"location":"configuration/06-platform-identity/#example-use-case","text":"Imagine that a project with resource on an OpenStack cloud wishes to host an open workshop using Jupyter notebooks for teaching. In this case, they definitely don't want to grant every workshop attendee access to OpenStack, as these users may not be trusted. Using Azimuth, a trusted project member can deploy a JupyterHub in Azimuth (i.e. as a Platform Admin) and create users in their Identity Realm for the workshop attendees. These users can be granted access to JupyterHub for the duration of the workshop, and at the end of the workshop their access can be revoked.","title":"Example use case"},{"location":"configuration/06-platform-identity/#use-of-keycloak","text":"In order to accomplish this, every Azimuth installation includes an instance of the Keycloak open-source identity management platform. Each Azimuth tenancy (i.e. OpenStack project) has an associated realm in this Keycloak instance that is used to manage access to platforms deployed in that tenancy. The realm provides authentication and authorization for the Zenith services associated with the platforms in the tenancy using OpenID Connect (OIDC) . Each realm is created with two groups - admins and platform-users . Users who are in the platform-users group for a realm are granted access to all platforms deployed in the corresponding Azimuth tenancy. Users who are in the admins group are granted admin status for the realm, meaning they can perform actions in the Keycloak admin console such as: Managing users Assigning users to groups Configuring authentication policies , e.g. password requirements, multi-factor authentication Integrating external identity providers The Keycloak realms created by Azimuth are configured with Azimuth as an identity provider, so that Platform Admins who belong to a tenancy in Azimuth can sign in to the corresponding realm using the Azimuth identity provider. Platform Admins who sign in to a realm via Azimuth are automatically placed in the admins and platform-users groups described above. Note This means that all users that can access the Azimuth tenancy by signing in with an OpenStack account (i.e. Platform Admins) automatically get access to all the platforms in the tenancy. This matches the behaviour of the previous shared cookie authentication, so existing Azimuth users will still be able to use their existing workflows. However the new approach uses an industry standard protocol designed specifically for federated identity in an untrusted environment, and as such eliminates a potential security flaw in the previous shared cookie approach that could leak unscoped OpenStack tokens to a malicious application exposed via Zenith. When a platform is deployed in an Azimuth tenancy a group is created in the corresponding identity realm, and access to the platform is controlled by membership of this group. Each Zenith service for the platform has a child group under the platform group that can be used to grant access to a single service within a platform. For example, the standard Linux Workstation platform (see Cluster-as-a-Service (CaaS) ) exposes two Zenith services - \"Web Console\" and \"Monitoring\". If an instance of this platform is deployed with the name my-workstation , then access to the \"Web Console\" service can be granted by either: Adding the user to the platform-users group. This will also grant the user access to all other platforms in the tenancy. Adding the user to the caas-my-workstation group. This will grant the user access to the \"Web Console\" and \"Monitoring\" services. Adding the user to the caas-my-workstation/webconsole group. This will grant the user access to the \"Web Console\" service only.","title":"Use of Keycloak"},{"location":"configuration/06-platform-identity/#keycloak-admin-password","text":"The only required configuration for platform identity is to set the admin password for Keycloak: environments/my-site/inventory/group_vars/all/secrets.yml keycloak_admin_password : \"<secure password>\" Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted .","title":"Keycloak admin password"},{"location":"configuration/07-zenith/","text":"Zenith Application Proxy The Zenith application proxy is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_apps_enabled : no The only piece of configuration that is required for Zenith is a secret key that is used to sign and verify the single-use tokens issued by the registrar (see the Zenith architecture document for details): environments/my-site/inventory/group_vars/all/secrets.yml zenith_registrar_subdomain_token_signing_key : \"<some secret key>\" Tip This key should be a long, random string - at least 32 bytes (256 bits) is recommended. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted . SSHD port number By default, the Zenith SSHD server will use port 22 on a dedicated IP address for a HA deployment and port 2222 on the pre-allocated floating IP for a single node deployment (port 22 is used for regular SSH to configure the node). This can be changed using the following variable, if required: environments/my-site/inventory/group_vars/all/variables.yml zenith_sshd_service_port : 22222","title":"Zenith Application Proxy"},{"location":"configuration/07-zenith/#zenith-application-proxy","text":"The Zenith application proxy is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_apps_enabled : no The only piece of configuration that is required for Zenith is a secret key that is used to sign and verify the single-use tokens issued by the registrar (see the Zenith architecture document for details): environments/my-site/inventory/group_vars/all/secrets.yml zenith_registrar_subdomain_token_signing_key : \"<some secret key>\" Tip This key should be a long, random string - at least 32 bytes (256 bits) is recommended. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Zenith Application Proxy"},{"location":"configuration/07-zenith/#sshd-port-number","text":"By default, the Zenith SSHD server will use port 22 on a dedicated IP address for a HA deployment and port 2222 on the pre-allocated floating IP for a single node deployment (port 22 is used for regular SSH to configure the node). This can be changed using the following variable, if required: environments/my-site/inventory/group_vars/all/variables.yml zenith_sshd_service_port : 22222","title":"SSHD port number"},{"location":"configuration/08-community-images/","text":"Community images Azimuth requires a number of specialised images to be available on the target cloud for Cluster-as-a-Service appliances and Kubernetes clusters. Images for the workstation and repo2docker appliances are built using Packer from the definitions in the azimuth-images repository . The results of these builds are uploaded here for consumption by Azimuth deployments. Similarly, images for the Slurm cluster appliance are built using Packer from definitions in the slurm-image-builder respository , with builds uploaded here . For Kubernetes, we use the OSISM builds of the Cluster API images from the Kubernetes image-builder . azimuth-ops is able to download, convert (if required) and then create Glance images on the target cloud from these sources. Images are uploaded as community images meaning they are accessible by all projects in the target cloud, but not included by default when listing images in Horizon, the OpenStack CLI or the OpenStack API. By default, azimuth-ops creates a set of images that are needed to support Azimuth's functionality and auto-wires them into the correct places for the K3S node, the HA Kubernetes cluster, the default Kubernetes templates and the default Cluster-as-a-Service appliances. Referencing uploaded images The IDs of the uploaded images are made available in the variable community_images_image_ids , which can be used to refer to the images elsewhere in your Azimuth configuration, e.g.: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_workstation_image : \"{{ community_images_image_ids.workstation_20220711_2135 }}\" Image conversion azimuth-ops is able to convert images from a \"source\" format to a \"target\" format for the target cloud. The majority of images available for download are in qcow2 format, and this is the format in which images built from azimuth-images are distributed. However your cloud may require images to be uploaded in a different format, e.g. raw or vmdk . To specify the target format for your cloud, just set the following (the default is qcow2 ): environments/my-site/inventory/group_vars/all/variables.yml community_images_disk_format : raw When this differs from the source format for an image, azimuth-ops will convert the image using qemu-img convert before uploading it to the target cloud. Disabling community images It is possible to prevent azimuth-ops from uploading any images, even the default ones, by setting the following: environments/my-site/inventory/group_vars/all/variables.yml community_images : {} Warning If community images are disabled you will need to ensure suitable images are uploaded via another mechanism, and the correct variables populated with the image IDs in your Azimuth configuration. See the azimuth-ops roles for more details. Custom images If you want to upload custom images as part of your Azimuth installation, for example to support a custom Cluster-as-a-Service appliance or for older Kubernetes versions, you can use the community_images_extra variable: environments/my-site/inventory/group_vars/all/variables.yml community_images_extra : debian_11_20220711_1073 : name : debian-11-generic-amd64-20220711-1073 source_url : https://cloud.debian.org/images/cloud/bullseye/20220711-1073/debian-11-generic-amd64-20220711-1073.qcow2 source_disk_format : qcow2 container_format : bare The ID of this image can then be referred to elsewhere in your Azimuth configuration using: environments/my-site/inventory/group_vars/all/variables.yml my_custom_caas_appliance_image : \"{{ community_images_image_ids.debian_11_20220711_1073 }}\" Warning The image specifications in community_images_extra are assumed to be immutable , i.e. they will never change. It is also assumed that the names refer to unique images. azimuth-ops will only download, convert and upload images to the target cloud for names that do not already exist. For this reason, it is recommended to include a timestamp or build reference in the image name in order to identify different images that serve the same purpose (e.g. different builds of the same pipeline).","title":"Community images"},{"location":"configuration/08-community-images/#community-images","text":"Azimuth requires a number of specialised images to be available on the target cloud for Cluster-as-a-Service appliances and Kubernetes clusters. Images for the workstation and repo2docker appliances are built using Packer from the definitions in the azimuth-images repository . The results of these builds are uploaded here for consumption by Azimuth deployments. Similarly, images for the Slurm cluster appliance are built using Packer from definitions in the slurm-image-builder respository , with builds uploaded here . For Kubernetes, we use the OSISM builds of the Cluster API images from the Kubernetes image-builder . azimuth-ops is able to download, convert (if required) and then create Glance images on the target cloud from these sources. Images are uploaded as community images meaning they are accessible by all projects in the target cloud, but not included by default when listing images in Horizon, the OpenStack CLI or the OpenStack API. By default, azimuth-ops creates a set of images that are needed to support Azimuth's functionality and auto-wires them into the correct places for the K3S node, the HA Kubernetes cluster, the default Kubernetes templates and the default Cluster-as-a-Service appliances.","title":"Community images"},{"location":"configuration/08-community-images/#referencing-uploaded-images","text":"The IDs of the uploaded images are made available in the variable community_images_image_ids , which can be used to refer to the images elsewhere in your Azimuth configuration, e.g.: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_workstation_image : \"{{ community_images_image_ids.workstation_20220711_2135 }}\"","title":"Referencing uploaded images"},{"location":"configuration/08-community-images/#image-conversion","text":"azimuth-ops is able to convert images from a \"source\" format to a \"target\" format for the target cloud. The majority of images available for download are in qcow2 format, and this is the format in which images built from azimuth-images are distributed. However your cloud may require images to be uploaded in a different format, e.g. raw or vmdk . To specify the target format for your cloud, just set the following (the default is qcow2 ): environments/my-site/inventory/group_vars/all/variables.yml community_images_disk_format : raw When this differs from the source format for an image, azimuth-ops will convert the image using qemu-img convert before uploading it to the target cloud.","title":"Image conversion"},{"location":"configuration/08-community-images/#disabling-community-images","text":"It is possible to prevent azimuth-ops from uploading any images, even the default ones, by setting the following: environments/my-site/inventory/group_vars/all/variables.yml community_images : {} Warning If community images are disabled you will need to ensure suitable images are uploaded via another mechanism, and the correct variables populated with the image IDs in your Azimuth configuration. See the azimuth-ops roles for more details.","title":"Disabling community images"},{"location":"configuration/08-community-images/#custom-images","text":"If you want to upload custom images as part of your Azimuth installation, for example to support a custom Cluster-as-a-Service appliance or for older Kubernetes versions, you can use the community_images_extra variable: environments/my-site/inventory/group_vars/all/variables.yml community_images_extra : debian_11_20220711_1073 : name : debian-11-generic-amd64-20220711-1073 source_url : https://cloud.debian.org/images/cloud/bullseye/20220711-1073/debian-11-generic-amd64-20220711-1073.qcow2 source_disk_format : qcow2 container_format : bare The ID of this image can then be referred to elsewhere in your Azimuth configuration using: environments/my-site/inventory/group_vars/all/variables.yml my_custom_caas_appliance_image : \"{{ community_images_image_ids.debian_11_20220711_1073 }}\" Warning The image specifications in community_images_extra are assumed to be immutable , i.e. they will never change. It is also assumed that the names refer to unique images. azimuth-ops will only download, convert and upload images to the target cloud for names that do not already exist. For this reason, it is recommended to include a timestamp or build reference in the image name in order to identify different images that serve the same purpose (e.g. different builds of the same pipeline).","title":"Custom images"},{"location":"configuration/09-kubernetes/","text":"Kubernetes Kubernetes support in Azimuth is implemented using Cluster API with the OpenStack provider . Support for cluster addons is provided by the Cluster API addon provider , which provides functionality for installing Helm charts and additional manifests. Azimuth provides an opinionated interface on top of Cluster API by implementing its own Kubernetes operator . This operator exposes two custom resources which are used by the Azimuth API to manage Kubernetes clusters: clustertemplates.azimuth.stackhpc.com A cluster template represents a \"type\" of Kubernetes cluster. In particular, this is used to provide different Kubernetes versions, but can also be used to provide advanced options, e.g. networking configuration or additional addons that are installed by default on the cluster. Cluster templates can be deprecated, e.g. when a new Kubernetes version is released, resulting in a warning being shown to the user that they should upgrade. clusters.azimuth.stackhpc.com A cluster represents the user-facing definition of a Kubernetes cluster. It references a template, from which the Kubernetes version and other advanced options are taken, but allows the user to specify one or more node groups and toggle a few simple options such as auto-healing and whether the monitoring stack is deployed on the cluster. For each Cluster , the operator manages a release of the openstack-cluster Helm chart . The Helm release in turn manages Cluster API resources for the cluster, including addons. To get the values for the release, the operator first derives some values from the Cluster object which are merged with the values defined in the referenced template. The result of that merge is then merged with any global configuration that has been specified before being passed to Helm. Tip The azimuth_capi_operator role defines a number of variables that can be set to control the behaviour of tenant Kubernetes clusters provisioned using Azimuth. In general, any of the options available for the openstack-cluster Helm chart can be set using either the azimuth_capi_operator_capi_helm_values_overrides variable (for global configuration) or the values section for a specific Kubernetes template. However the default values should be sufficient for most deployments of Azimuth. Disabling Kubernetes Kubernetes support is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_kubernetes_enabled : no Multiple external networks In the case where multiple external networks are available to tenants, you must tell Azimuth which one to use for floating IPs for Kubernetes services in tenant clusters: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_external_network_id : \"<network id>\" Images When building a cluster, Cluster API requires that an image exists in the target cloud, accessible to the target project, that has the correct version of kubelet and kubeadm available. azimuth-ops is able to upload suitable images using the Community images functionality . If you would prefer to manage the images using another mechanism, suitable images can be built using the Kubernetes image-builder . The ID of the image for a particular Kubernetes version must be given in the cluster template. Availability zones By default, an Azimuth installation assumes that there is a single availability zone (AZ) called nova - this is the default set up and common for small-to-medium sized clouds. If this is not the case for your target cloud, you can set some variables to determine the availability zones that are used for Kubernetes nodes. The possible options are discussed in Availability Zones for Kubernetes nodes . The relevant variables are: environments/my-site/inventory/group_vars/all/variables.yml # Indicates whether to omit the failure domain (AZ) from control plane nodes azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain : true # The AZs to consider for control plane nodes # Only used if the flag above is false azimuth_capi_operator_capi_helm_control_plane_failure_domains : [ az1 , az2 ] # The AZ to use for workers azimuth_capi_operator_capi_helm_worker_failure_domain : az1 # Set to null to omit the AZ from worker nodes azimuth_capi_operator_capi_helm_worker_failure_domain : null Cluster templates azimuth-ops is able to manage the available Kubernetes cluster templates using the variable azimuth_capi_operator_cluster_templates . This variable is a dictionary that maps cluster template names to their specifications, and represents the current (i.e. not deprecated) cluster templates. azimuth-ops will not remove cluster templates, but it will mark any templates that are not present in this variable as deprecated. By default, azimuth-ops will ensure a cluster template is present for the latest patch version of each Kubernetes release that is currently maintained. These templates are configured so that Kubernetes nodes will go onto the Azimuth portal-internal network for the project in which the cluster is being deployed. Disabling the default templates To disable the default templates, just set the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_default : {} Custom cluster templates If you want to include custom cluster templates in addition to the default templates, e.g. for advanced networking configurations, you can specify them using the variable azimuth_capi_operator_cluster_templates_extra . For example, the following demonstrates how to configure a template where the cluster worker nodes have two networks attached - the control plane nodes and workers are all attached to the Azimuth internal network but the workers are attached to an additional SR-IOV capable network: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_extra : # The index in the dict is the template name kube-1-24-2-multinet : # A human-readable label for the template label : v1.24.2 / multinet # A brief description of the template description : >- Kubernetes 1.24.2 with HA control plane and high-performance networking. # Values for the openstack-cluster Helm chart values : # Specify the image and version for the cluster # These are the only required values kubernetesVersion : 1.24.2 machineImageId : \"{{ community_images_image_ids.kube_1_24_2 }}\" # Use the portal-internal network as the main cluster network clusterNetworking : internalNetwork : networkFilter : tags : portal-internal # Configure an extra SR-IOV port on worker nodes using an SR-IOV capable network nodeGroupDefaults : machineNetworking : ports : - {} - network : tags : sriov-vlan securityGroups : [] vnicType : direct Harbor registry azimuth-ops is able to manage a Harbor registry as part of an Azimuth installation, which is primarily used as a proxy cache to avoid pulling images from the internet. The Harbor registry will be made available at registry.<ingress base domain> , e.g. registry.apps.example.org . azimuth-ops will auto-wire use of the Harbor registry into tenant Kubernetes clusters for registries that have a proxy cache defined. Harbor is enabled by default, and requires two secrets to be set: environments/my-site/inventory/group_vars/all/secrets.yml # The admin password for Harbor harbor_admin_password : \"<secure password>\" # The secret key for Harbor # This MUST be exactly 16 alphanumeric characters harbor_secret_key : \"<secure secret key>\" Danger These values should be kept secret. If you want to keep them in Git - which is recommended - then they must be encrypted . Disabling Harbor The Harbor registry can be disabled entirely: environments/my-site/inventory/group_vars/all/variables.yml harbor_enabled : no Additional proxy caches By default, a single proxy cache is defined for Docker Hub in order to get around the rate limit . Note Additional default proxy caches may be added in the future for common public repositories, e.g. quay.io . You can also define additional proxy caches for other registries: environments/my-site/inventory/group_vars/all/variables.yml harbor_proxy_cache_extra_projects : quay.io : # The name of the project in Harbor name : quay-public # The type of the upstream registry, e.g.: # aws-ecr # azure-acr # docker-hub # docker-registry # gitlab # google-gcr # harbor # jfrog-artifactory # quay-io type : quay-io # The endpoint URL for the registry url : https://quay.io Warning Currently, azimuth-ops does not support defining authentication for the upstream registries, i.e. only public repositories are supported.","title":"Kubernetes"},{"location":"configuration/09-kubernetes/#kubernetes","text":"Kubernetes support in Azimuth is implemented using Cluster API with the OpenStack provider . Support for cluster addons is provided by the Cluster API addon provider , which provides functionality for installing Helm charts and additional manifests. Azimuth provides an opinionated interface on top of Cluster API by implementing its own Kubernetes operator . This operator exposes two custom resources which are used by the Azimuth API to manage Kubernetes clusters: clustertemplates.azimuth.stackhpc.com A cluster template represents a \"type\" of Kubernetes cluster. In particular, this is used to provide different Kubernetes versions, but can also be used to provide advanced options, e.g. networking configuration or additional addons that are installed by default on the cluster. Cluster templates can be deprecated, e.g. when a new Kubernetes version is released, resulting in a warning being shown to the user that they should upgrade. clusters.azimuth.stackhpc.com A cluster represents the user-facing definition of a Kubernetes cluster. It references a template, from which the Kubernetes version and other advanced options are taken, but allows the user to specify one or more node groups and toggle a few simple options such as auto-healing and whether the monitoring stack is deployed on the cluster. For each Cluster , the operator manages a release of the openstack-cluster Helm chart . The Helm release in turn manages Cluster API resources for the cluster, including addons. To get the values for the release, the operator first derives some values from the Cluster object which are merged with the values defined in the referenced template. The result of that merge is then merged with any global configuration that has been specified before being passed to Helm. Tip The azimuth_capi_operator role defines a number of variables that can be set to control the behaviour of tenant Kubernetes clusters provisioned using Azimuth. In general, any of the options available for the openstack-cluster Helm chart can be set using either the azimuth_capi_operator_capi_helm_values_overrides variable (for global configuration) or the values section for a specific Kubernetes template. However the default values should be sufficient for most deployments of Azimuth.","title":"Kubernetes"},{"location":"configuration/09-kubernetes/#disabling-kubernetes","text":"Kubernetes support is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_kubernetes_enabled : no","title":"Disabling Kubernetes"},{"location":"configuration/09-kubernetes/#multiple-external-networks","text":"In the case where multiple external networks are available to tenants, you must tell Azimuth which one to use for floating IPs for Kubernetes services in tenant clusters: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_external_network_id : \"<network id>\"","title":"Multiple external networks"},{"location":"configuration/09-kubernetes/#images","text":"When building a cluster, Cluster API requires that an image exists in the target cloud, accessible to the target project, that has the correct version of kubelet and kubeadm available. azimuth-ops is able to upload suitable images using the Community images functionality . If you would prefer to manage the images using another mechanism, suitable images can be built using the Kubernetes image-builder . The ID of the image for a particular Kubernetes version must be given in the cluster template.","title":"Images"},{"location":"configuration/09-kubernetes/#availability-zones","text":"By default, an Azimuth installation assumes that there is a single availability zone (AZ) called nova - this is the default set up and common for small-to-medium sized clouds. If this is not the case for your target cloud, you can set some variables to determine the availability zones that are used for Kubernetes nodes. The possible options are discussed in Availability Zones for Kubernetes nodes . The relevant variables are: environments/my-site/inventory/group_vars/all/variables.yml # Indicates whether to omit the failure domain (AZ) from control plane nodes azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain : true # The AZs to consider for control plane nodes # Only used if the flag above is false azimuth_capi_operator_capi_helm_control_plane_failure_domains : [ az1 , az2 ] # The AZ to use for workers azimuth_capi_operator_capi_helm_worker_failure_domain : az1 # Set to null to omit the AZ from worker nodes azimuth_capi_operator_capi_helm_worker_failure_domain : null","title":"Availability zones"},{"location":"configuration/09-kubernetes/#cluster-templates","text":"azimuth-ops is able to manage the available Kubernetes cluster templates using the variable azimuth_capi_operator_cluster_templates . This variable is a dictionary that maps cluster template names to their specifications, and represents the current (i.e. not deprecated) cluster templates. azimuth-ops will not remove cluster templates, but it will mark any templates that are not present in this variable as deprecated. By default, azimuth-ops will ensure a cluster template is present for the latest patch version of each Kubernetes release that is currently maintained. These templates are configured so that Kubernetes nodes will go onto the Azimuth portal-internal network for the project in which the cluster is being deployed.","title":"Cluster templates"},{"location":"configuration/09-kubernetes/#disabling-the-default-templates","text":"To disable the default templates, just set the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_default : {}","title":"Disabling the default templates"},{"location":"configuration/09-kubernetes/#custom-cluster-templates","text":"If you want to include custom cluster templates in addition to the default templates, e.g. for advanced networking configurations, you can specify them using the variable azimuth_capi_operator_cluster_templates_extra . For example, the following demonstrates how to configure a template where the cluster worker nodes have two networks attached - the control plane nodes and workers are all attached to the Azimuth internal network but the workers are attached to an additional SR-IOV capable network: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_extra : # The index in the dict is the template name kube-1-24-2-multinet : # A human-readable label for the template label : v1.24.2 / multinet # A brief description of the template description : >- Kubernetes 1.24.2 with HA control plane and high-performance networking. # Values for the openstack-cluster Helm chart values : # Specify the image and version for the cluster # These are the only required values kubernetesVersion : 1.24.2 machineImageId : \"{{ community_images_image_ids.kube_1_24_2 }}\" # Use the portal-internal network as the main cluster network clusterNetworking : internalNetwork : networkFilter : tags : portal-internal # Configure an extra SR-IOV port on worker nodes using an SR-IOV capable network nodeGroupDefaults : machineNetworking : ports : - {} - network : tags : sriov-vlan securityGroups : [] vnicType : direct","title":"Custom cluster templates"},{"location":"configuration/09-kubernetes/#harbor-registry","text":"azimuth-ops is able to manage a Harbor registry as part of an Azimuth installation, which is primarily used as a proxy cache to avoid pulling images from the internet. The Harbor registry will be made available at registry.<ingress base domain> , e.g. registry.apps.example.org . azimuth-ops will auto-wire use of the Harbor registry into tenant Kubernetes clusters for registries that have a proxy cache defined. Harbor is enabled by default, and requires two secrets to be set: environments/my-site/inventory/group_vars/all/secrets.yml # The admin password for Harbor harbor_admin_password : \"<secure password>\" # The secret key for Harbor # This MUST be exactly 16 alphanumeric characters harbor_secret_key : \"<secure secret key>\" Danger These values should be kept secret. If you want to keep them in Git - which is recommended - then they must be encrypted .","title":"Harbor registry"},{"location":"configuration/09-kubernetes/#disabling-harbor","text":"The Harbor registry can be disabled entirely: environments/my-site/inventory/group_vars/all/variables.yml harbor_enabled : no","title":"Disabling Harbor"},{"location":"configuration/09-kubernetes/#additional-proxy-caches","text":"By default, a single proxy cache is defined for Docker Hub in order to get around the rate limit . Note Additional default proxy caches may be added in the future for common public repositories, e.g. quay.io . You can also define additional proxy caches for other registries: environments/my-site/inventory/group_vars/all/variables.yml harbor_proxy_cache_extra_projects : quay.io : # The name of the project in Harbor name : quay-public # The type of the upstream registry, e.g.: # aws-ecr # azure-acr # docker-hub # docker-registry # gitlab # google-gcr # harbor # jfrog-artifactory # quay-io type : quay-io # The endpoint URL for the registry url : https://quay.io Warning Currently, azimuth-ops does not support defining authentication for the upstream registries, i.e. only public repositories are supported.","title":"Additional proxy caches"},{"location":"configuration/10-kubernetes-apps/","text":"Kubernetes apps Azimuth allows operators to provide a catalog of applications (apps) that users are able to install onto their Kubernetes clusters via the Azimuth user interface. Multiple apps can be installed on the same Kubernetes cluster, and each app gets its own namespace. A Kubernetes app in Azimuth essentially consists of a Helm chart . Azimuth manages specially annotated instances of the HelmRelease resource from the Cluster API addon provider to install and upgrade apps on the target cluster. These apps can be integrated with Zenith to expose services to the user without requiring the use of LoadBalancer services or Kubernetes ingress . The available apps and the available versions of those apps are determined by instances of a custom resource provided by the Azimuth Kubernetes operator - apptemplates.azimuth.stackhpc.com - which references a chart in a Helm chart repository . Azimuth uses the chart metadata to generate the user interface for the app template, and the values schema for the chart to generate a form for collecting input from the user. Azimuth also renders the output of the NOTES.txt file in the user interface, so this can be used to describe how to consume the application. Warning If the chart does not have a values schema, the generated form will be blank and the chart will be deployed with the default values. Tip In addition, a file called azimuth-ui.schema.yaml can be included to apply some small customisations to the generated form, like selecting different controls. See the azimuth-charts for examples. Default app templates Azimuth comes with two app templates enabled by default: jupyterhub Allows the user to deploy JupyterHub on their clusters. JupyterHub provides a multi-user environment for using Jupyter notebooks where each user gets their own dynamically-provisioned notebook server and storage. The Jupyter notebook interface is exposed using Zenith . daskhub A JupyterHub instance with Dask integration. Dask is a library that aims to simplify the process of scaling data-intensive Python applications, such as those using Numpy or pandas . DaskHub installs Dask Gateway alongside JupyterHub and configures them so that they integrate seamlessly. This allows users to easily create Dask clusters in their notebooks that scale out by creating pods on the underlying Kubernetes cluster. As with jupyterhub above, the notebook interface is exposed using Zenith. These can be disabled by setting the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_jupyterhub_enabled : false azimuth_capi_operator_app_templates_daskhub_enabled : false Custom app templates If you have Helm charts that you want to make available as apps, you can define them as follows: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_extra : # The key is the name of the app template my-custom-app : # This is the only required field, and determines the chart that is used chart : # The chart repository containing the chart repo : https://my.company.org/helm-charts # The name of the chart that the template is for name : my-custom-app By default, Azimuth will use the last 5 stable versions of the chart (i.e. versions without a prerelease part, see semver.org ) and the name , icon and description from the Chart.yaml file in the chart to build the user interface. A chart annotation is also supported to define the human-readable label: my-chart/Chart.yaml annotations : azimuth.stackhpc.com/label : My Custom App This behaviour can be customised for each app template using the following optional fields: label The human-readable label for the app template, instead of the annotation from Chart.yaml . logo The URL of the logo for the app template, instead of the icon from Chart.yaml . description A short description of the app template, instead of the description from Chart.yaml . versionRange Default: >=0.0.0 (stable versions only). The range of chart versions to consider. Must be a comma-separated list of constraints, where each constraint is an operator followed by a SemVer version . The supported operators are == , != , > , >= , < and <= . Prerelease versions are only considered if the lower bound includes a prerelease part. If no lower bound is given, an implicit lower bound of >=0.0.0 is used. Some examples of valid ranges: >=0.0.0-0 - all versions, including prerelease versions like 1.0.0-alpha.1 < 2.0.0 - all stable versions matching 0.X.Y and 1.X.Y (implicit lower bound) >=1.0.0, < 3.0.0,!=2.1.5 - all stable versions matching 1.X.Y or 2.X.Y except 2.1.5 (useful for excluding specific versions with known issues) keepVersions Default: 5. The number of versions to keep. This is used to limit the size of the AppTemplate resource, because etcd has limits on the maximum size of objects (usually approx. 1MB). This will need to be smaller if the chart has a large values schema. syncFrequency Default: 86400 (24 hours). The number of seconds to wait before checking for new versions of the chart. defaultValues Default: {} . Default values for deployments of the app, on top of the chart defaults. Zenith integration When Zenith is enabled , every Kubernetes cluster created by Azimuth has an instance of the Zenith operator watching it. This operator makes two Kubernetes custom resources available that can be used to expose a Kubernetes service to users without using type: NodePort , type: LoadBalancer or Kubernetes ingress : reservations.zenith.stackhpc.com Represents the reservation of a Zenith domain, and results in the generation of a Kubernetes secret containing an SSH keypair associated with the allocated domain. clients.zenith.stackhpc.com Defines a Zenith client, pointing at a Zenith reservation and upstream Kubernetes service. In addition, services can benefit from the project-level authentication and authorization that is performed by Zenith at it's proxy to prevent unauthorised users from accessing the service. The Zenith services associated with an app are monitored and made available in the Azimuth user interface, making the apps easy to use. Azimuth supports the following annotations on the reservation to determine how the service is rendered in the user interface: annotations : azimuth.stackhpc.com/service-label : \"My Fancy Service\" azimuth.stackhpc.com/service-icon-url : https://my.company.org/images/my-fancy-service-icon.png It is possible to add Zenith integration to an existing chart by creating a new parent chart with the existing chart as a dependency . You can define templates for the Zenith resources in the parent chart, pointing at services created by the child chart. This is the approach taken by the azimuth-charts that provide the default jupyterhub and daskhub apps.","title":"Kubernetes apps"},{"location":"configuration/10-kubernetes-apps/#kubernetes-apps","text":"Azimuth allows operators to provide a catalog of applications (apps) that users are able to install onto their Kubernetes clusters via the Azimuth user interface. Multiple apps can be installed on the same Kubernetes cluster, and each app gets its own namespace. A Kubernetes app in Azimuth essentially consists of a Helm chart . Azimuth manages specially annotated instances of the HelmRelease resource from the Cluster API addon provider to install and upgrade apps on the target cluster. These apps can be integrated with Zenith to expose services to the user without requiring the use of LoadBalancer services or Kubernetes ingress . The available apps and the available versions of those apps are determined by instances of a custom resource provided by the Azimuth Kubernetes operator - apptemplates.azimuth.stackhpc.com - which references a chart in a Helm chart repository . Azimuth uses the chart metadata to generate the user interface for the app template, and the values schema for the chart to generate a form for collecting input from the user. Azimuth also renders the output of the NOTES.txt file in the user interface, so this can be used to describe how to consume the application. Warning If the chart does not have a values schema, the generated form will be blank and the chart will be deployed with the default values. Tip In addition, a file called azimuth-ui.schema.yaml can be included to apply some small customisations to the generated form, like selecting different controls. See the azimuth-charts for examples.","title":"Kubernetes apps"},{"location":"configuration/10-kubernetes-apps/#default-app-templates","text":"Azimuth comes with two app templates enabled by default: jupyterhub Allows the user to deploy JupyterHub on their clusters. JupyterHub provides a multi-user environment for using Jupyter notebooks where each user gets their own dynamically-provisioned notebook server and storage. The Jupyter notebook interface is exposed using Zenith . daskhub A JupyterHub instance with Dask integration. Dask is a library that aims to simplify the process of scaling data-intensive Python applications, such as those using Numpy or pandas . DaskHub installs Dask Gateway alongside JupyterHub and configures them so that they integrate seamlessly. This allows users to easily create Dask clusters in their notebooks that scale out by creating pods on the underlying Kubernetes cluster. As with jupyterhub above, the notebook interface is exposed using Zenith. These can be disabled by setting the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_jupyterhub_enabled : false azimuth_capi_operator_app_templates_daskhub_enabled : false","title":"Default app templates"},{"location":"configuration/10-kubernetes-apps/#custom-app-templates","text":"If you have Helm charts that you want to make available as apps, you can define them as follows: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_extra : # The key is the name of the app template my-custom-app : # This is the only required field, and determines the chart that is used chart : # The chart repository containing the chart repo : https://my.company.org/helm-charts # The name of the chart that the template is for name : my-custom-app By default, Azimuth will use the last 5 stable versions of the chart (i.e. versions without a prerelease part, see semver.org ) and the name , icon and description from the Chart.yaml file in the chart to build the user interface. A chart annotation is also supported to define the human-readable label: my-chart/Chart.yaml annotations : azimuth.stackhpc.com/label : My Custom App This behaviour can be customised for each app template using the following optional fields: label The human-readable label for the app template, instead of the annotation from Chart.yaml . logo The URL of the logo for the app template, instead of the icon from Chart.yaml . description A short description of the app template, instead of the description from Chart.yaml . versionRange Default: >=0.0.0 (stable versions only). The range of chart versions to consider. Must be a comma-separated list of constraints, where each constraint is an operator followed by a SemVer version . The supported operators are == , != , > , >= , < and <= . Prerelease versions are only considered if the lower bound includes a prerelease part. If no lower bound is given, an implicit lower bound of >=0.0.0 is used. Some examples of valid ranges: >=0.0.0-0 - all versions, including prerelease versions like 1.0.0-alpha.1 < 2.0.0 - all stable versions matching 0.X.Y and 1.X.Y (implicit lower bound) >=1.0.0, < 3.0.0,!=2.1.5 - all stable versions matching 1.X.Y or 2.X.Y except 2.1.5 (useful for excluding specific versions with known issues) keepVersions Default: 5. The number of versions to keep. This is used to limit the size of the AppTemplate resource, because etcd has limits on the maximum size of objects (usually approx. 1MB). This will need to be smaller if the chart has a large values schema. syncFrequency Default: 86400 (24 hours). The number of seconds to wait before checking for new versions of the chart. defaultValues Default: {} . Default values for deployments of the app, on top of the chart defaults.","title":"Custom app templates"},{"location":"configuration/10-kubernetes-apps/#zenith-integration","text":"When Zenith is enabled , every Kubernetes cluster created by Azimuth has an instance of the Zenith operator watching it. This operator makes two Kubernetes custom resources available that can be used to expose a Kubernetes service to users without using type: NodePort , type: LoadBalancer or Kubernetes ingress : reservations.zenith.stackhpc.com Represents the reservation of a Zenith domain, and results in the generation of a Kubernetes secret containing an SSH keypair associated with the allocated domain. clients.zenith.stackhpc.com Defines a Zenith client, pointing at a Zenith reservation and upstream Kubernetes service. In addition, services can benefit from the project-level authentication and authorization that is performed by Zenith at it's proxy to prevent unauthorised users from accessing the service. The Zenith services associated with an app are monitored and made available in the Azimuth user interface, making the apps easy to use. Azimuth supports the following annotations on the reservation to determine how the service is rendered in the user interface: annotations : azimuth.stackhpc.com/service-label : \"My Fancy Service\" azimuth.stackhpc.com/service-icon-url : https://my.company.org/images/my-fancy-service-icon.png It is possible to add Zenith integration to an existing chart by creating a new parent chart with the existing chart as a dependency . You can define templates for the Zenith resources in the parent chart, pointing at services created by the child chart. This is the approach taken by the azimuth-charts that provide the default jupyterhub and daskhub apps.","title":"Zenith integration"},{"location":"configuration/11-caas/","text":"Cluster-as-a-Service (CaaS) Cluster-as-a-Service (CaaS) is enabled by default in the reference documentation. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_clusters_enabled : no As discussed in the Azimuth architecture document, the appliances exposed to users via the Azimuth UI are determined by projects and job templates in AWX . As used by CaaS, a project is essentially a Git repository containing Ansible playbooks, and the job templates correspond to individual playbooks within those projects. It is entirely possible to configure the available appliances using only the AWX UI. However azimuth-ops allows you to define the available appliances using Ansible variables. AWX admin password The only required configuration for CaaS is to set the admin password for AWX: environments/my-site/inventory/group_vars/all/secrets.yml awx_admin_password : \"<secure password>\" Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted . StackHPC Appliances By default, three appliances maintained by StackHPC are made available - the Slurm appliance , the Linux Workstation appliance and the repo2docker appliance . Slurm appliance The Slurm appliance allows users to deploy Slurm clusters for running batch workloads. The clusters include the Open OnDemand web interface and a monitoring stack with web dashboards, both of which are exposed using Zenith. To disable the Slurm appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_slurm_appliance_enabled : no The Slurm appliance requires the following configuration: environments/my-site/inventory/group_vars/all/variables.yml # The name of a flavor to use for Slurm login nodes # A flavor with at least 2 CPUs and 4GB RAM should be used azimuth_caas_stackhpc_slurm_appliance_login_flavor_name : \"<flavor name>\" # The name of a flavor to use for Slurm control nodes # A flavor with at least 2 CPUs and 4GB RAM should be used azimuth_caas_stackhpc_slurm_appliance_control_flavor_name : \"<flavor name>\" Linux Workstation appliance The Linux Workstation appliance allows users to provision a workstation that is accessible via a web-browser using Apache Guacamole . Guacamole provides a web-based virtual desktop and a console, and is exposed using Zenith. A simple monitoring stack is also available, exposed via Zenith. To disable the Linux Workstation appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_workstation_enabled : no repo2docker appliance The repo2docker appliance allows users to deploy a Jupyter Notebook server, exposed via Zenith, from a repo2docker compliant repository. A simple monitoring stack is also available, exposed via Zenith. To disable the repo2docker appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_repo2docker_enabled : no Custom appliances It is possible to make custom appliances available in the Azimuth interface for users to deploy. For more information on building a CaaS-compatible appliance, please see the sample appliance . Custom appliances can be specified with the following configuration: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_awx_extra_projects : - # The name of the appliance project in AWX name : StackHPC Sample Appliance # The git URL of the appliance gitUrl : https://github.com/stackhpc/azimuth-sample-appliance.git # The branch, tag or commit id to use # For production, it is recommended to use a fixed tag or commit ID gitVersion : master # The base URL for cluster metadata files metadataRoot : https://raw.githubusercontent.com/stackhpc/azimuth-sample-appliance/{gitVersion}/ui-meta # List of playbooks that correspond to appliances playbooks : [ sample-appliance.yml ] # Dict of extra variables for appliances # The keys are the playbooks # The values are maps of Ansible extra_vars for those playbooks # The special key __ALL__ can be used to set common extra_vars for # all playbooks in a project extraVars : __ALL__ : # Use the ID of an Ubuntu 20.04 image that we asked azimuth-ops to upload cluster_image : \"{{ community_images_image_ids.ubuntu_2004_20220712 }}\"","title":"Cluster-as-a-Service (CaaS)"},{"location":"configuration/11-caas/#cluster-as-a-service-caas","text":"Cluster-as-a-Service (CaaS) is enabled by default in the reference documentation. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_clusters_enabled : no As discussed in the Azimuth architecture document, the appliances exposed to users via the Azimuth UI are determined by projects and job templates in AWX . As used by CaaS, a project is essentially a Git repository containing Ansible playbooks, and the job templates correspond to individual playbooks within those projects. It is entirely possible to configure the available appliances using only the AWX UI. However azimuth-ops allows you to define the available appliances using Ansible variables.","title":"Cluster-as-a-Service (CaaS)"},{"location":"configuration/11-caas/#awx-admin-password","text":"The only required configuration for CaaS is to set the admin password for AWX: environments/my-site/inventory/group_vars/all/secrets.yml awx_admin_password : \"<secure password>\" Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted .","title":"AWX admin password"},{"location":"configuration/11-caas/#stackhpc-appliances","text":"By default, three appliances maintained by StackHPC are made available - the Slurm appliance , the Linux Workstation appliance and the repo2docker appliance .","title":"StackHPC Appliances"},{"location":"configuration/11-caas/#slurm-appliance","text":"The Slurm appliance allows users to deploy Slurm clusters for running batch workloads. The clusters include the Open OnDemand web interface and a monitoring stack with web dashboards, both of which are exposed using Zenith. To disable the Slurm appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_slurm_appliance_enabled : no The Slurm appliance requires the following configuration: environments/my-site/inventory/group_vars/all/variables.yml # The name of a flavor to use for Slurm login nodes # A flavor with at least 2 CPUs and 4GB RAM should be used azimuth_caas_stackhpc_slurm_appliance_login_flavor_name : \"<flavor name>\" # The name of a flavor to use for Slurm control nodes # A flavor with at least 2 CPUs and 4GB RAM should be used azimuth_caas_stackhpc_slurm_appliance_control_flavor_name : \"<flavor name>\"","title":"Slurm appliance"},{"location":"configuration/11-caas/#linux-workstation-appliance","text":"The Linux Workstation appliance allows users to provision a workstation that is accessible via a web-browser using Apache Guacamole . Guacamole provides a web-based virtual desktop and a console, and is exposed using Zenith. A simple monitoring stack is also available, exposed via Zenith. To disable the Linux Workstation appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_workstation_enabled : no","title":"Linux Workstation appliance"},{"location":"configuration/11-caas/#repo2docker-appliance","text":"The repo2docker appliance allows users to deploy a Jupyter Notebook server, exposed via Zenith, from a repo2docker compliant repository. A simple monitoring stack is also available, exposed via Zenith. To disable the repo2docker appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_repo2docker_enabled : no","title":"repo2docker appliance"},{"location":"configuration/11-caas/#custom-appliances","text":"It is possible to make custom appliances available in the Azimuth interface for users to deploy. For more information on building a CaaS-compatible appliance, please see the sample appliance . Custom appliances can be specified with the following configuration: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_awx_extra_projects : - # The name of the appliance project in AWX name : StackHPC Sample Appliance # The git URL of the appliance gitUrl : https://github.com/stackhpc/azimuth-sample-appliance.git # The branch, tag or commit id to use # For production, it is recommended to use a fixed tag or commit ID gitVersion : master # The base URL for cluster metadata files metadataRoot : https://raw.githubusercontent.com/stackhpc/azimuth-sample-appliance/{gitVersion}/ui-meta # List of playbooks that correspond to appliances playbooks : [ sample-appliance.yml ] # Dict of extra variables for appliances # The keys are the playbooks # The values are maps of Ansible extra_vars for those playbooks # The special key __ALL__ can be used to set common extra_vars for # all playbooks in a project extraVars : __ALL__ : # Use the ID of an Ubuntu 20.04 image that we asked azimuth-ops to upload cluster_image : \"{{ community_images_image_ids.ubuntu_2004_20220712 }}\"","title":"Custom appliances"},{"location":"configuration/12-monitoring/","text":"Monitoring and alerting Azimuth installations come with a monitoring and alerting stack that uses Prometheus to collect metrics on various components of the Kubernetes cluster and the Azimuth components running on it, Alertmanager to produce alerts based on those metrics and Grafana to visualise the metrics using a curated set of dashboards. HA installations also include a log aggregation stack using Loki and Promtail that collects logs from all the pods running on the cluster and the systemd services on each cluster node. These logs are available in a dashboard in Grafana, where they can be filtered and searched. Persistence and retention Note Persistence is only configured for HA deployments. In order for metrics, alert state (e.g. silences) and logs to persist across pod restarts, we must configure Prometheus, Alertmanager and Loki to use persistent volumes to store their data. This is configured by default in an Azimuth HA installation, but you may wish to tweak the retention periods and/or volume sizes based on your requirements and/or observed usage. The following variables, shown with their default values, control the retention periods and volume sizes for Alertmanager, Prometheus and Loki: environments/my-site/inventory/group_vars/all/variables.yml # Alertmanager retention and volume size capi_cluster_addons_monitoring_alertmanager_retention : 168h capi_cluster_addons_monitoring_alertmanager_volume_size : 10Gi # Prometheus retention and volume size capi_cluster_addons_monitoring_prometheus_retention : 90d capi_cluster_addons_monitoring_prometheus_volume_size : 10Gi # Loki retention and volume size capi_cluster_addons_monitoring_loki_retention : 744h capi_cluster_addons_monitoring_loki_volume_size : 10Gi Danger Volumes can only be increased in size. Any attempt to reduce the size of a volume will be rejected. Slack alerts If your organisation uses Slack , Alertmanager can be configured to send alerts to a Slack channel using an Incoming Webhook . To enable Slack alerts, you must first create a webhook . This should result in a URL of the form: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX This URL should be placed in the following variable to allow Azimuth's Alertmanager to send alerts to Slack: environments/my-site/inventory/group_vars/all/secrets.yml alertmanager_config_slack_webhook_url : https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX Danger The webhook URL should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Monitoring and alerting"},{"location":"configuration/12-monitoring/#monitoring-and-alerting","text":"Azimuth installations come with a monitoring and alerting stack that uses Prometheus to collect metrics on various components of the Kubernetes cluster and the Azimuth components running on it, Alertmanager to produce alerts based on those metrics and Grafana to visualise the metrics using a curated set of dashboards. HA installations also include a log aggregation stack using Loki and Promtail that collects logs from all the pods running on the cluster and the systemd services on each cluster node. These logs are available in a dashboard in Grafana, where they can be filtered and searched.","title":"Monitoring and alerting"},{"location":"configuration/12-monitoring/#persistence-and-retention","text":"Note Persistence is only configured for HA deployments. In order for metrics, alert state (e.g. silences) and logs to persist across pod restarts, we must configure Prometheus, Alertmanager and Loki to use persistent volumes to store their data. This is configured by default in an Azimuth HA installation, but you may wish to tweak the retention periods and/or volume sizes based on your requirements and/or observed usage. The following variables, shown with their default values, control the retention periods and volume sizes for Alertmanager, Prometheus and Loki: environments/my-site/inventory/group_vars/all/variables.yml # Alertmanager retention and volume size capi_cluster_addons_monitoring_alertmanager_retention : 168h capi_cluster_addons_monitoring_alertmanager_volume_size : 10Gi # Prometheus retention and volume size capi_cluster_addons_monitoring_prometheus_retention : 90d capi_cluster_addons_monitoring_prometheus_volume_size : 10Gi # Loki retention and volume size capi_cluster_addons_monitoring_loki_retention : 744h capi_cluster_addons_monitoring_loki_volume_size : 10Gi Danger Volumes can only be increased in size. Any attempt to reduce the size of a volume will be rejected.","title":"Persistence and retention"},{"location":"configuration/12-monitoring/#slack-alerts","text":"If your organisation uses Slack , Alertmanager can be configured to send alerts to a Slack channel using an Incoming Webhook . To enable Slack alerts, you must first create a webhook . This should result in a URL of the form: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX This URL should be placed in the following variable to allow Azimuth's Alertmanager to send alerts to Slack: environments/my-site/inventory/group_vars/all/secrets.yml alertmanager_config_slack_webhook_url : https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX Danger The webhook URL should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Slack alerts"},{"location":"debugging/","text":"Debugging Azimuth This section provides tips on debugging some common issues with the components of an Azimuth installation. All of these debugging tips assume that you have activated the environment that you want to debug using: source ./bin/activate my-site","title":"Debugging Azimuth"},{"location":"debugging/#debugging-azimuth","text":"This section provides tips on debugging some common issues with the components of an Azimuth installation. All of these debugging tips assume that you have activated the environment that you want to debug using: source ./bin/activate my-site","title":"Debugging Azimuth"},{"location":"debugging/access-ha/","text":"Accessing the HA cluster HA clusters are provisioned using Cluster API on the K3S node. The Kubernetes API of the HA cluster is not accessible to the internet, so the K3S node is used to access it. On the K3S node, a kubeconfig file for the HA cluster is created in the $HOME directory of the ubuntu user. You can activate this kubeconfig by setting the KUBECONFIG environment variable, which allows you to access the HA cluster using kubectl : $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ export KUBECONFIG=./kubeconfig-azimuth-staging.yaml ubuntu@azimuth-staging-seed:~$ kubectl get po -n azimuth NAME READY STATUS RESTARTS AGE awx-operator-controller-manager-7b4d9ffddd-k7bsq 2/2 Running 0 55m awx-d864c46f8-bx6db 4/4 Running 0 51m awx-postgres-0 1/1 Running 0 52m azimuth-api-6847fcd6c8-746pc 1/1 Running 0 55m azimuth-capi-operator-789b5c8f44-2g4wx 1/1 Running 0 55m azimuth-ui-fc556-cjwq2 1/1 Running 0 55m consul-8zbnx 1/1 Running 0 55m consul-kf8fl 1/1 Running 0 55m consul-n4w2x 1/1 Running 0 55m consul-server-0 1/1 Running 0 55m consul-server-1 1/1 Running 0 55m consul-server-2 1/1 Running 0 55m zenith-registrar-86df769979-z2cgb 1/1 Running 0 55m zenith-sshd-768794b88b-48rcf 1/1 Running 0 55m zenith-sync-8f55f978c-v6czg 1/1 Running 0 55m","title":"Accessing the HA cluster"},{"location":"debugging/access-ha/#accessing-the-ha-cluster","text":"HA clusters are provisioned using Cluster API on the K3S node. The Kubernetes API of the HA cluster is not accessible to the internet, so the K3S node is used to access it. On the K3S node, a kubeconfig file for the HA cluster is created in the $HOME directory of the ubuntu user. You can activate this kubeconfig by setting the KUBECONFIG environment variable, which allows you to access the HA cluster using kubectl : $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ export KUBECONFIG=./kubeconfig-azimuth-staging.yaml ubuntu@azimuth-staging-seed:~$ kubectl get po -n azimuth NAME READY STATUS RESTARTS AGE awx-operator-controller-manager-7b4d9ffddd-k7bsq 2/2 Running 0 55m awx-d864c46f8-bx6db 4/4 Running 0 51m awx-postgres-0 1/1 Running 0 52m azimuth-api-6847fcd6c8-746pc 1/1 Running 0 55m azimuth-capi-operator-789b5c8f44-2g4wx 1/1 Running 0 55m azimuth-ui-fc556-cjwq2 1/1 Running 0 55m consul-8zbnx 1/1 Running 0 55m consul-kf8fl 1/1 Running 0 55m consul-n4w2x 1/1 Running 0 55m consul-server-0 1/1 Running 0 55m consul-server-1 1/1 Running 0 55m consul-server-2 1/1 Running 0 55m zenith-registrar-86df769979-z2cgb 1/1 Running 0 55m zenith-sshd-768794b88b-48rcf 1/1 Running 0 55m zenith-sync-8f55f978c-v6czg 1/1 Running 0 55m","title":"Accessing the HA cluster"},{"location":"debugging/access-k3s/","text":"Accessing the K3S cluster Both the single node and high-availability (HA) deployment methods have a K3S node that is provisioned using Terraform. In the single node case, this is the cluster that actually hosts Azimuth and all its dependencies. In the HA case, this cluster is configured as a Cluster API management cluster for the HA cluster. In both cases, the K3S node is deployed using Terraform and the IP address and SSH key for accessing the node are in the Terraform state for the environment. The azimuth-config repository contains a utility script - seed-ssh - that will extract these details from the Terraform state for the active environment and use them to execute an SSH command to access the provisioned node. Once on the node, you can use kubectl to inspect the state of the Kubernetes cluster. It is already configured with the correct kubeconfig file: $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7796b77cd4-mk9gw 1/1 Running 0 2d kube-system metrics-server-ff9dbcb6c-nbpjt 1/1 Running 0 2d cert-manager cert-manager-webhook-5fd7d458f7-scwd8 1/1 Running 0 2d kube-system local-path-provisioner-84bb864455-r4vd2 1/1 Running 0 2d cert-manager cert-manager-66b6d6bf59-vzqrq 1/1 Running 0 2d capi-system capi-controller-manager-7f45d4b75b-47cf4 1/1 Running 0 2d capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-5f5d7fb49-684n9 1/1 Running 0 2d cert-manager cert-manager-cainjector-856d4df858-bgs4d 1/1 Running 0 2d capo-system capo-controller-manager-5c9748574f-vnbzp 1/1 Running 0 2d capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-c444455b5-zn4qw 1/1 Running 0 2d default azimuth-staging-addons-cloud-config-install-9eeff--1-694x4 0/1 Completed 0 2d default azimuth-staging-addons-cni-calico-install-0d58f--1-cjwzx 0/1 Completed 0 2d default azimuth-staging-addons-ccm-openstack-install-bc2b0--1-jlb4b 0/1 Completed 0 2d default azimuth-staging-addons-prometheus-operator-crds-instal--1-sn66s 0/1 Completed 0 2d default azimuth-staging-addons-metrics-server-install-45390--1-rr5m6 0/1 Completed 0 2d default azimuth-staging-addons-csi-cinder-install-53888--1-cg9gq 0/1 Completed 0 2d default azimuth-staging-addons-ingress-nginx-install-478f3--1-p4hwb 0/1 Completed 0 2d default azimuth-staging-addons-loki-stack-install-c8cd1--1-5z2wc 0/1 Completed 0 2d default azimuth-staging-addons-kube-prometheus-stack-install-4--1-xgmvd 0/1 Completed 0 2d default azimuth-staging-autoscaler-66cc55487c-qfpgn 1/1 Running 0 2d","title":"Accessing the K3S cluster"},{"location":"debugging/access-k3s/#accessing-the-k3s-cluster","text":"Both the single node and high-availability (HA) deployment methods have a K3S node that is provisioned using Terraform. In the single node case, this is the cluster that actually hosts Azimuth and all its dependencies. In the HA case, this cluster is configured as a Cluster API management cluster for the HA cluster. In both cases, the K3S node is deployed using Terraform and the IP address and SSH key for accessing the node are in the Terraform state for the environment. The azimuth-config repository contains a utility script - seed-ssh - that will extract these details from the Terraform state for the active environment and use them to execute an SSH command to access the provisioned node. Once on the node, you can use kubectl to inspect the state of the Kubernetes cluster. It is already configured with the correct kubeconfig file: $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7796b77cd4-mk9gw 1/1 Running 0 2d kube-system metrics-server-ff9dbcb6c-nbpjt 1/1 Running 0 2d cert-manager cert-manager-webhook-5fd7d458f7-scwd8 1/1 Running 0 2d kube-system local-path-provisioner-84bb864455-r4vd2 1/1 Running 0 2d cert-manager cert-manager-66b6d6bf59-vzqrq 1/1 Running 0 2d capi-system capi-controller-manager-7f45d4b75b-47cf4 1/1 Running 0 2d capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-5f5d7fb49-684n9 1/1 Running 0 2d cert-manager cert-manager-cainjector-856d4df858-bgs4d 1/1 Running 0 2d capo-system capo-controller-manager-5c9748574f-vnbzp 1/1 Running 0 2d capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-c444455b5-zn4qw 1/1 Running 0 2d default azimuth-staging-addons-cloud-config-install-9eeff--1-694x4 0/1 Completed 0 2d default azimuth-staging-addons-cni-calico-install-0d58f--1-cjwzx 0/1 Completed 0 2d default azimuth-staging-addons-ccm-openstack-install-bc2b0--1-jlb4b 0/1 Completed 0 2d default azimuth-staging-addons-prometheus-operator-crds-instal--1-sn66s 0/1 Completed 0 2d default azimuth-staging-addons-metrics-server-install-45390--1-rr5m6 0/1 Completed 0 2d default azimuth-staging-addons-csi-cinder-install-53888--1-cg9gq 0/1 Completed 0 2d default azimuth-staging-addons-ingress-nginx-install-478f3--1-p4hwb 0/1 Completed 0 2d default azimuth-staging-addons-loki-stack-install-c8cd1--1-5z2wc 0/1 Completed 0 2d default azimuth-staging-addons-kube-prometheus-stack-install-4--1-xgmvd 0/1 Completed 0 2d default azimuth-staging-autoscaler-66cc55487c-qfpgn 1/1 Running 0 2d","title":"Accessing the K3S cluster"},{"location":"debugging/access-monitoring/","text":"Accessing the monitoring The monitoring is currently only exposed inside the cluster, so it can only be accessed using kubectl port-forward from the K3S node. However because the API is not accessible to the internet, an SSH local forward must be used from your local machine to the K3S node as well. To simplify this process, the azimuth-config repository contains a utility script - port-forward - that can be used to set up the double port-forward for particular cluster services. To view monitoring dashboards in Grafana, use the following command to expose the Grafana interface on a local port: ./bin/port-forward monitoring 3000 This will make the Grafana interface available at http://localhost:3000 . Log in with the default credentials - admin/prom-operator - to access the dashboards. In order to view firing alerts or configure silences, you can also access the Prometheus and Alertmanager interfaces using the same method: ./bin/port-forward prometheus 9090 ./bin/port-forward alertmanager 9093 These commands will expose the Prometheus and Alertmanager interfaces at http://localhost:9090 and http://localhost:9093 respectively. Both these interfaces are unauthenticated, although you must have sufficient access to set up the port forward via the K3S node.","title":"Accessing the monitoring"},{"location":"debugging/access-monitoring/#accessing-the-monitoring","text":"The monitoring is currently only exposed inside the cluster, so it can only be accessed using kubectl port-forward from the K3S node. However because the API is not accessible to the internet, an SSH local forward must be used from your local machine to the K3S node as well. To simplify this process, the azimuth-config repository contains a utility script - port-forward - that can be used to set up the double port-forward for particular cluster services. To view monitoring dashboards in Grafana, use the following command to expose the Grafana interface on a local port: ./bin/port-forward monitoring 3000 This will make the Grafana interface available at http://localhost:3000 . Log in with the default credentials - admin/prom-operator - to access the dashboards. In order to view firing alerts or configure silences, you can also access the Prometheus and Alertmanager interfaces using the same method: ./bin/port-forward prometheus 9090 ./bin/port-forward alertmanager 9093 These commands will expose the Prometheus and Alertmanager interfaces at http://localhost:9090 and http://localhost:9093 respectively. Both these interfaces are unauthenticated, although you must have sufficient access to set up the port forward via the K3S node.","title":"Accessing the monitoring"},{"location":"debugging/caas/","text":"Debugging Cluster-as-a-Service In order to debug issues with CaaS deployments, it is often useful to access the AWX UI. Similar to the monitoring, this interface is only accessible inside the cluster. To access it, use the following command: ./bin/port-forward awx 8052 The AWX UI will then be available at http://localhost:8052 . Sign in to AWX as the admin user with the password that you set in Configuring CaaS . Once inside, you can look at the details of the recently executed jobs, check that the inventories look correct and check the permissions assigned to teams. Tip You can also make changes in the AWX UI that will be reflected in the cluster types and clusters available in Azimuth. However make sure you know what you are doing!","title":"Debugging Cluster-as-a-Service"},{"location":"debugging/caas/#debugging-cluster-as-a-service","text":"In order to debug issues with CaaS deployments, it is often useful to access the AWX UI. Similar to the monitoring, this interface is only accessible inside the cluster. To access it, use the following command: ./bin/port-forward awx 8052 The AWX UI will then be available at http://localhost:8052 . Sign in to AWX as the admin user with the password that you set in Configuring CaaS . Once inside, you can look at the details of the recently executed jobs, check that the inventories look correct and check the permissions assigned to teams. Tip You can also make changes in the AWX UI that will be reflected in the cluster types and clusters available in Azimuth. However make sure you know what you are doing!","title":"Debugging Cluster-as-a-Service"},{"location":"debugging/kubernetes/","text":"Debugging Kubernetes As described in Configuring Kubernetes , Azimuth uses Cluster API to manage tenant Kubernetes clusters. Cluster API resources are managed by releases of the openstack-cluster Helm chart , which in turn are managed by the azimuth-capi-operator in response to changes to instances of the clusters.azimuth.stackhpc.com custom resource. These instances are created, updated and deleted in Kubernetes by the Azimuth API in response to user actions. The Azimuth API creates a namespace for each project, in which cluster resources are created. These namespaces are of the form az-<sanitized project name> . It is also important to note that the Kubernetes API servers for tenant clusters do not use Octavia load balancers like the Azimuth HA cluster. Instead, the API servers for tenant clusters are exposed via Zenith. When issues occur with cluster provisioning, here are some things to try in order to locate the issue. Cluster resource exists First, check if the cluster resource exists in the tenant namespace: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster NAME LABEL TEMPLATE KUBERNETES VERSION PHASE NODE COUNT AGE demo demo kube-1-24-2 1.24.2 Ready 4 11d If no cluster resource exists, check if the Kubernetes CRDs are installed: On the K3S node, targetting the HA cluster if deployed $ kubectl get crd | grep azimuth apptemplates.azimuth.stackhpc.com 2022-11-02T11:11:13Z clusters.azimuth.stackhpc.com 2022-11-02T10:53:26Z clustertemplates.azimuth.stackhpc.com 2022-11-02T10:53:26Z If they do not exist, check if the azimuth-capi-operator is running: On the K3S node, targetting the HA cluster if deployed $ kubectl -n azimuth get po -l app.kubernetes.io/instance=azimuth-capi-operator NAME READY STATUS RESTARTS AGE azimuth-capi-operator-5c65c4b598-h2thx 1/1 Running 0 10d Helm release exists The first thing that the azimuth-capi-operator does when it sees a new cluster resource is make a Helm release: On the K3S node, targetting the HA cluster if deployed $ helm -n az-demo list -a NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION demo az-demo 1 2022-07-07 13:26:22.94084961 +0000 UTC deployed openstack-cluster-0.1.0-dev.0.main.161 a0bcee5 If the Helm release does not exist, restart the azimuth-capi-operator : On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deploy/azimuth-capi-operator If the Helm release does not get created, even after a restart, check the logs of the azimuth-capi-operator for any warnings or errors: On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/azimuth-capi-operator [ -f ] Cluster API resource status If the Helm release is in the deployed status, the next thing to check is the state of the Cluster API resources that were created: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster-api NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION AGE manifests.addons.stackhpc.com/demo-cloud-config demo true openstack-system cloud-config Deployed 1 11d manifests.addons.stackhpc.com/demo-csi-cinder-storageclass demo true openstack-system csi-cinder-storageclass Deployed 1 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-client demo true monitoring-system kube-prometheus-stack-client Deployed 2 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-dashboards demo true monitoring-system kube-prometheus-stack-dashboards Deployed 1 11d manifests.addons.stackhpc.com/demo-kubernetes-dashboard-client demo true kubernetes-dashboard kubernetes-dashboard-client Deployed 2 11d manifests.addons.stackhpc.com/demo-loki-stack-dashboards demo true monitoring-system loki-stack-dashboards Deployed 1 11d NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION CHART NAME CHART VERSION AGE helmrelease.addons.stackhpc.com/dask-demo demo dask-demo dask-demo Deployed 1 daskhub-azimuth 0.1.0-dev.0.main.23 11d helmrelease.addons.stackhpc.com/demo-ccm-openstack demo true openstack-system ccm-openstack Deployed 1 openstack-cloud-controller-manager 1.3.0 11d helmrelease.addons.stackhpc.com/demo-cni-calico demo true tigera-operator cni-calico Deployed 1 tigera-operator v3.23.3 11d helmrelease.addons.stackhpc.com/demo-csi-cinder demo true openstack-system csi-cinder Deployed 1 openstack-cinder-csi 2.2.0 11d helmrelease.addons.stackhpc.com/demo-kube-prometheus-stack demo true monitoring-system kube-prometheus-stack Deployed 1 kube-prometheus-stack 40.1.0 11d helmrelease.addons.stackhpc.com/demo-kubernetes-dashboard demo true kubernetes-dashboard kubernetes-dashboard Deployed 1 kubernetes-dashboard 5.10.0 11d helmrelease.addons.stackhpc.com/demo-loki-stack demo true monitoring-system loki-stack Deployed 1 loki-stack 2.8.2 11d helmrelease.addons.stackhpc.com/demo-mellanox-network-operator demo true network-operator mellanox-network-operator Deployed 1 network-operator 1.3.0 11d helmrelease.addons.stackhpc.com/demo-metrics-server demo true kube-system metrics-server Deployed 1 metrics-server 3.8.2 11d helmrelease.addons.stackhpc.com/demo-node-feature-discovery demo true node-feature-discovery node-feature-discovery Deployed 1 node-feature-discovery 0.11.2 11d helmrelease.addons.stackhpc.com/demo-nvidia-gpu-operator demo true gpu-operator nvidia-gpu-operator Deployed 1 gpu-operator v1.11.1 11d NAME CLUSTER AGE kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-5kjjt demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-897r6 demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-x26zz demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d-lddk6 demo 11d NAME AGE kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d 11d NAME CLUSTER EXPECTEDMACHINES MAXUNHEALTHY CURRENTHEALTHY AGE machinehealthcheck.cluster.x-k8s.io/demo-control-plane demo 3 100% 3 11d machinehealthcheck.cluster.x-k8s.io/demo-sm0 demo 1 100% 1 11d NAME CLUSTER REPLICAS READY AVAILABLE AGE VERSION machineset.cluster.x-k8s.io/demo-sm0-c99fb7798 demo 1 1 1 11d v1.24.2 NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION machinedeployment.cluster.x-k8s.io/demo-sm0 demo 1 1 1 0 Running 11d v1.24.2 NAME PHASE AGE VERSION cluster.cluster.x-k8s.io/demo Provisioned 11d NAME CLUSTER NODENAME PROVIDERID PHASE AGE VERSION machine.cluster.x-k8s.io/demo-control-plane-7p8zv demo demo-control-plane-7d76d0be-z6dm8 openstack:///f687f926-3cee-4550-91e5-32c2885708b0 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-9skvh demo demo-control-plane-7d76d0be-d2mcr openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-s8dhv demo demo-control-plane-7d76d0be-j64w6 openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-sm0-c99fb7798-qqk4j demo demo-sm0-7d76d0be-gdjwv openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 Running 11d v1.24.2 NAME CLUSTER INITIALIZED API SERVER AVAILABLE REPLICAS READY UPDATED UNAVAILABLE AGE VERSION kubeadmcontrolplane.controlplane.cluster.x-k8s.io/demo-control-plane demo true true 3 3 3 0 11d v1.24.2 NAME CLUSTER READY NETWORK SUBNET BASTION IP openstackcluster.infrastructure.cluster.x-k8s.io/demo demo true 4b6b2722-ee5b-40ec-8e52-a6610e14cc51 73e22c49-10b8-4763-af2f-4c0cce007c82 NAME CLUSTER INSTANCESTATE READY PROVIDERID MACHINE openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-d2mcr demo ACTIVE true openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c demo-control-plane-9skvh openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-j64w6 demo ACTIVE true openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 demo-control-plane-s8dhv openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-z6dm8 demo ACTIVE true openstack:///f687f926-3cee-4550-91e5-32c2885708b0 demo-control-plane-7p8zv openstackmachine.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be-gdjwv demo ACTIVE true openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 demo-sm0-c99fb7798-qqk4j NAME AGE openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be 11d openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be 11d The cluster.cluster.x-k8s.io resource should be Provisioned , the machine.cluster.x-k8s.io resources should be Running with an associated NODENAME , the openstackmachine.infrastructure.cluster.x-k8s.io resources should be ACTIVE and the {manifests,helmrelease}.addons.stackhpc.com resources should all be Deployed . If this is not the case, first check the interactive console of the cluster nodes in Horizon to see if the nodes had any problems joining the cluster. Also check to see if the Zenith service for the API server was created correctly - once all control plane nodes have registered correctly the Endpoints resource for the service should have an entry for each control plane node (usually three). If these all look OK, check the logs of the Cluster API providers for any errors: On the K3S node, targetting the HA cluster if deployed kubectl -n capi-system logs deploy/capi-controller-manager kubectl -n capi-kubeadm-bootstrap-system logs deploy/capi-kubeadm-bootstrap-controller-manager kubectl -n capi-kubeadm-control-plane-system logs deploy/capi-kubeadm-control-plane-controller-manager kubectl -n capo-system logs deploy/capo-controller-manager kubectl -n capi-addon-system logs deploy/cluster-api-addon-provider Zenith service issues Zenith services are enabled on Kubernetes clusters using the Zenith operator . Each tenant Kubernetes cluster gets an instance of the operator that runs on the Azimuth cluster, where it can reach the Zenith registrar to allocate subdomains, but watches the tenant cluster for instances of the reservation.zenith.stackhpc.com and client.zenith.stackhpc.com custom resources. By creating instances of these resources in the tenant cluster, Kubernetes Services in the target cluster can be exposed via Zenith. If Zenith services are not becoming available for Kubernetes cluster services, first follow the procedure for debugging a Zenith service , including checking that the clients were created correctly and that the pods are running: Targetting the tenant cluster $ kubectl get zenith -A NAMESPACE NAME PHASE UPSTREAM SERVICE MITM ENABLED MITM AUTH AGE kubernetes-dashboard client.zenith.stackhpc.com/kubernetes-dashboard Available kubernetes-dashboard true ServiceAccount 4d19h monitoring-system client.zenith.stackhpc.com/kube-prometheus-stack Available kube-prometheus-stack-grafana true Basic 4d19h NAMESPACE NAME SECRET PHASE FQDN AGE kubernetes-dashboard reservation.zenith.stackhpc.com/kubernetes-dashboard kubernetes-dashboard-zenith-credential Ready mwqgcdrk77nva18uzcct3g7jlo7obi7zlbcgemuhk6nhk.apps.example.org 4d19h monitoring-system reservation.zenith.stackhpc.com/kube-prometheus-stack kube-prometheus-stack-zenith-credential Ready zovdsnnesww2hiw074mvufvcfgczfbd2yhmuhsf3p59xa.apps.example.org 4d19h $ kubectl get deploy,po -A -l app.kubernetes.io/managed-by=zenith-operator NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-dashboard deployment.apps/kubernetes-dashboard-zenith-client 1/1 1 1 4d19h monitoring-system deployment.apps/kube-prometheus-stack-zenith-client 1/1 1 1 4d19h NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard pod/kubernetes-dashboard-zenith-client-86c5fd9bd-2jfdb 2/2 Running 0 4d19h monitoring-system pod/kube-prometheus-stack-zenith-client-b9986579d-qgp82 2/2 Running 0 9h Tip The kubeconfig for a tenant cluster is available in a secret in the tenant namespace: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo get secret | grep kubeconfig demo-kubeconfig cluster.x-k8s.io/secret 1 11d If everything looks OK, try restarting the Zenith operator for the cluster: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo rollout restart deploy/demo-zenith-operator","title":"Debugging Kubernetes"},{"location":"debugging/kubernetes/#debugging-kubernetes","text":"As described in Configuring Kubernetes , Azimuth uses Cluster API to manage tenant Kubernetes clusters. Cluster API resources are managed by releases of the openstack-cluster Helm chart , which in turn are managed by the azimuth-capi-operator in response to changes to instances of the clusters.azimuth.stackhpc.com custom resource. These instances are created, updated and deleted in Kubernetes by the Azimuth API in response to user actions. The Azimuth API creates a namespace for each project, in which cluster resources are created. These namespaces are of the form az-<sanitized project name> . It is also important to note that the Kubernetes API servers for tenant clusters do not use Octavia load balancers like the Azimuth HA cluster. Instead, the API servers for tenant clusters are exposed via Zenith. When issues occur with cluster provisioning, here are some things to try in order to locate the issue.","title":"Debugging Kubernetes"},{"location":"debugging/kubernetes/#cluster-resource-exists","text":"First, check if the cluster resource exists in the tenant namespace: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster NAME LABEL TEMPLATE KUBERNETES VERSION PHASE NODE COUNT AGE demo demo kube-1-24-2 1.24.2 Ready 4 11d If no cluster resource exists, check if the Kubernetes CRDs are installed: On the K3S node, targetting the HA cluster if deployed $ kubectl get crd | grep azimuth apptemplates.azimuth.stackhpc.com 2022-11-02T11:11:13Z clusters.azimuth.stackhpc.com 2022-11-02T10:53:26Z clustertemplates.azimuth.stackhpc.com 2022-11-02T10:53:26Z If they do not exist, check if the azimuth-capi-operator is running: On the K3S node, targetting the HA cluster if deployed $ kubectl -n azimuth get po -l app.kubernetes.io/instance=azimuth-capi-operator NAME READY STATUS RESTARTS AGE azimuth-capi-operator-5c65c4b598-h2thx 1/1 Running 0 10d","title":"Cluster resource exists"},{"location":"debugging/kubernetes/#helm-release-exists","text":"The first thing that the azimuth-capi-operator does when it sees a new cluster resource is make a Helm release: On the K3S node, targetting the HA cluster if deployed $ helm -n az-demo list -a NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION demo az-demo 1 2022-07-07 13:26:22.94084961 +0000 UTC deployed openstack-cluster-0.1.0-dev.0.main.161 a0bcee5 If the Helm release does not exist, restart the azimuth-capi-operator : On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deploy/azimuth-capi-operator If the Helm release does not get created, even after a restart, check the logs of the azimuth-capi-operator for any warnings or errors: On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/azimuth-capi-operator [ -f ]","title":"Helm release exists"},{"location":"debugging/kubernetes/#cluster-api-resource-status","text":"If the Helm release is in the deployed status, the next thing to check is the state of the Cluster API resources that were created: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster-api NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION AGE manifests.addons.stackhpc.com/demo-cloud-config demo true openstack-system cloud-config Deployed 1 11d manifests.addons.stackhpc.com/demo-csi-cinder-storageclass demo true openstack-system csi-cinder-storageclass Deployed 1 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-client demo true monitoring-system kube-prometheus-stack-client Deployed 2 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-dashboards demo true monitoring-system kube-prometheus-stack-dashboards Deployed 1 11d manifests.addons.stackhpc.com/demo-kubernetes-dashboard-client demo true kubernetes-dashboard kubernetes-dashboard-client Deployed 2 11d manifests.addons.stackhpc.com/demo-loki-stack-dashboards demo true monitoring-system loki-stack-dashboards Deployed 1 11d NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION CHART NAME CHART VERSION AGE helmrelease.addons.stackhpc.com/dask-demo demo dask-demo dask-demo Deployed 1 daskhub-azimuth 0.1.0-dev.0.main.23 11d helmrelease.addons.stackhpc.com/demo-ccm-openstack demo true openstack-system ccm-openstack Deployed 1 openstack-cloud-controller-manager 1.3.0 11d helmrelease.addons.stackhpc.com/demo-cni-calico demo true tigera-operator cni-calico Deployed 1 tigera-operator v3.23.3 11d helmrelease.addons.stackhpc.com/demo-csi-cinder demo true openstack-system csi-cinder Deployed 1 openstack-cinder-csi 2.2.0 11d helmrelease.addons.stackhpc.com/demo-kube-prometheus-stack demo true monitoring-system kube-prometheus-stack Deployed 1 kube-prometheus-stack 40.1.0 11d helmrelease.addons.stackhpc.com/demo-kubernetes-dashboard demo true kubernetes-dashboard kubernetes-dashboard Deployed 1 kubernetes-dashboard 5.10.0 11d helmrelease.addons.stackhpc.com/demo-loki-stack demo true monitoring-system loki-stack Deployed 1 loki-stack 2.8.2 11d helmrelease.addons.stackhpc.com/demo-mellanox-network-operator demo true network-operator mellanox-network-operator Deployed 1 network-operator 1.3.0 11d helmrelease.addons.stackhpc.com/demo-metrics-server demo true kube-system metrics-server Deployed 1 metrics-server 3.8.2 11d helmrelease.addons.stackhpc.com/demo-node-feature-discovery demo true node-feature-discovery node-feature-discovery Deployed 1 node-feature-discovery 0.11.2 11d helmrelease.addons.stackhpc.com/demo-nvidia-gpu-operator demo true gpu-operator nvidia-gpu-operator Deployed 1 gpu-operator v1.11.1 11d NAME CLUSTER AGE kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-5kjjt demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-897r6 demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-x26zz demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d-lddk6 demo 11d NAME AGE kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d 11d NAME CLUSTER EXPECTEDMACHINES MAXUNHEALTHY CURRENTHEALTHY AGE machinehealthcheck.cluster.x-k8s.io/demo-control-plane demo 3 100% 3 11d machinehealthcheck.cluster.x-k8s.io/demo-sm0 demo 1 100% 1 11d NAME CLUSTER REPLICAS READY AVAILABLE AGE VERSION machineset.cluster.x-k8s.io/demo-sm0-c99fb7798 demo 1 1 1 11d v1.24.2 NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION machinedeployment.cluster.x-k8s.io/demo-sm0 demo 1 1 1 0 Running 11d v1.24.2 NAME PHASE AGE VERSION cluster.cluster.x-k8s.io/demo Provisioned 11d NAME CLUSTER NODENAME PROVIDERID PHASE AGE VERSION machine.cluster.x-k8s.io/demo-control-plane-7p8zv demo demo-control-plane-7d76d0be-z6dm8 openstack:///f687f926-3cee-4550-91e5-32c2885708b0 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-9skvh demo demo-control-plane-7d76d0be-d2mcr openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-s8dhv demo demo-control-plane-7d76d0be-j64w6 openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-sm0-c99fb7798-qqk4j demo demo-sm0-7d76d0be-gdjwv openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 Running 11d v1.24.2 NAME CLUSTER INITIALIZED API SERVER AVAILABLE REPLICAS READY UPDATED UNAVAILABLE AGE VERSION kubeadmcontrolplane.controlplane.cluster.x-k8s.io/demo-control-plane demo true true 3 3 3 0 11d v1.24.2 NAME CLUSTER READY NETWORK SUBNET BASTION IP openstackcluster.infrastructure.cluster.x-k8s.io/demo demo true 4b6b2722-ee5b-40ec-8e52-a6610e14cc51 73e22c49-10b8-4763-af2f-4c0cce007c82 NAME CLUSTER INSTANCESTATE READY PROVIDERID MACHINE openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-d2mcr demo ACTIVE true openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c demo-control-plane-9skvh openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-j64w6 demo ACTIVE true openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 demo-control-plane-s8dhv openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-z6dm8 demo ACTIVE true openstack:///f687f926-3cee-4550-91e5-32c2885708b0 demo-control-plane-7p8zv openstackmachine.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be-gdjwv demo ACTIVE true openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 demo-sm0-c99fb7798-qqk4j NAME AGE openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be 11d openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be 11d The cluster.cluster.x-k8s.io resource should be Provisioned , the machine.cluster.x-k8s.io resources should be Running with an associated NODENAME , the openstackmachine.infrastructure.cluster.x-k8s.io resources should be ACTIVE and the {manifests,helmrelease}.addons.stackhpc.com resources should all be Deployed . If this is not the case, first check the interactive console of the cluster nodes in Horizon to see if the nodes had any problems joining the cluster. Also check to see if the Zenith service for the API server was created correctly - once all control plane nodes have registered correctly the Endpoints resource for the service should have an entry for each control plane node (usually three). If these all look OK, check the logs of the Cluster API providers for any errors: On the K3S node, targetting the HA cluster if deployed kubectl -n capi-system logs deploy/capi-controller-manager kubectl -n capi-kubeadm-bootstrap-system logs deploy/capi-kubeadm-bootstrap-controller-manager kubectl -n capi-kubeadm-control-plane-system logs deploy/capi-kubeadm-control-plane-controller-manager kubectl -n capo-system logs deploy/capo-controller-manager kubectl -n capi-addon-system logs deploy/cluster-api-addon-provider","title":"Cluster API resource status"},{"location":"debugging/kubernetes/#zenith-service-issues","text":"Zenith services are enabled on Kubernetes clusters using the Zenith operator . Each tenant Kubernetes cluster gets an instance of the operator that runs on the Azimuth cluster, where it can reach the Zenith registrar to allocate subdomains, but watches the tenant cluster for instances of the reservation.zenith.stackhpc.com and client.zenith.stackhpc.com custom resources. By creating instances of these resources in the tenant cluster, Kubernetes Services in the target cluster can be exposed via Zenith. If Zenith services are not becoming available for Kubernetes cluster services, first follow the procedure for debugging a Zenith service , including checking that the clients were created correctly and that the pods are running: Targetting the tenant cluster $ kubectl get zenith -A NAMESPACE NAME PHASE UPSTREAM SERVICE MITM ENABLED MITM AUTH AGE kubernetes-dashboard client.zenith.stackhpc.com/kubernetes-dashboard Available kubernetes-dashboard true ServiceAccount 4d19h monitoring-system client.zenith.stackhpc.com/kube-prometheus-stack Available kube-prometheus-stack-grafana true Basic 4d19h NAMESPACE NAME SECRET PHASE FQDN AGE kubernetes-dashboard reservation.zenith.stackhpc.com/kubernetes-dashboard kubernetes-dashboard-zenith-credential Ready mwqgcdrk77nva18uzcct3g7jlo7obi7zlbcgemuhk6nhk.apps.example.org 4d19h monitoring-system reservation.zenith.stackhpc.com/kube-prometheus-stack kube-prometheus-stack-zenith-credential Ready zovdsnnesww2hiw074mvufvcfgczfbd2yhmuhsf3p59xa.apps.example.org 4d19h $ kubectl get deploy,po -A -l app.kubernetes.io/managed-by=zenith-operator NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-dashboard deployment.apps/kubernetes-dashboard-zenith-client 1/1 1 1 4d19h monitoring-system deployment.apps/kube-prometheus-stack-zenith-client 1/1 1 1 4d19h NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard pod/kubernetes-dashboard-zenith-client-86c5fd9bd-2jfdb 2/2 Running 0 4d19h monitoring-system pod/kube-prometheus-stack-zenith-client-b9986579d-qgp82 2/2 Running 0 9h Tip The kubeconfig for a tenant cluster is available in a secret in the tenant namespace: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo get secret | grep kubeconfig demo-kubeconfig cluster.x-k8s.io/secret 1 11d If everything looks OK, try restarting the Zenith operator for the cluster: On the K3S node, targetting the HA cluster if deployed $ kubectl -n az-demo rollout restart deploy/demo-zenith-operator","title":"Zenith service issues"},{"location":"debugging/zenith-services/","text":"Debugging Zenith services If a Zenith service does not become available, the most common causes are: Client not registering correctly with SSHD To determine if this is the case, it is useful to access the Consul UI. Similar to the monitoring, this interface is only accessible inside the cluster. To access it, use the following command: ./bin/port-forward consul 3000 The Consul UI will then be available at http://localhost:3000 . The default view shows Consul's view of the services, where you can check if the service is being registered correctly. If the client is not correctly registering with SSHD, the first thing to do is check the logs for the client and restart it if necessary. If the client is failing to connect to SSHD, then try restarting SSHD: On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sshd Kubernetes resources for the Zenith service have not been created If the service exists in Consul, it is possible that the process that synchronises Consul services with Kubernetes resources is not functioning correctly. To check if Kubernetes resources are being created, run the following command and check that the Ingress , Service and Endpoints resources have been created for the service: On the K3S node, targetting the HA cluster if deployed $ kubectl -n zenith-services get ingress,service,endpoints NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g nginx cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g.apps.example.org 96.241.100.96 80, 443 2d ingress.networking.k8s.io/i03xvflgk1zmtcsdm2x5z5lz9qz05027euw nginx i03xvflgk1zmtcsdm2x5z5lz9qz05027euw.apps.example.org 96.241.100.96 80, 443 2d ingress.networking.k8s.io/pxmvy7235x2ggfvf2op615gvz2v59wkqglc nginx pxmvy7235x2ggfvf2op615gvz2v59wkqglc.apps.example.org 96.241.100.96 80, 443 2d ingress.networking.k8s.io/txn3zidfdnru5rg109voh848n51rvicmr1s nginx txn3zidfdnru5rg109voh848n51rvicmr1s.apps.example.org 96.241.100.96 80, 443 2d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g ClusterIP 172.27.10.109 <none> 80/TCP 2d service/i03xvflgk1zmtcsdm2x5z5lz9qz05027euw ClusterIP 172.30.203.227 <none> 80/TCP 2d service/pxmvy7235x2ggfvf2op615gvz2v59wkqglc ClusterIP 172.28.51.148 <none> 80/TCP 2d service/txn3zidfdnru5rg109voh848n51rvicmr1s ClusterIP 172.29.136.245 <none> 80/TCP 2d NAME ENDPOINTS AGE endpoints/cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g 172.18.152.99:34665 2d endpoints/i03xvflgk1zmtcsdm2x5z5lz9qz05027euw 172.18.152.99:39409 2d endpoints/pxmvy7235x2ggfvf2op615gvz2v59wkqglc 172.18.152.99:44761,172.18.152.99:36483,172.18.152.99:46449 2d endpoints/txn3zidfdnru5rg109voh848n51rvicmr1s 172.18.152.99:45379 2d Tip If an ingress resource does not have an IP, this may be a sign that the ingress controller is not correctly configured or not functioning correctly. If they do not exist, try restarting the Zenith sync component: On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sync cert-manager fails to obtain a certificate If you are using cert-manager to dynamically allocate certificates for Zenith services it is possible that cert-manager has failed to obtain a certificate for the service, e.g. because it has been rate-limited. To check if this is the case, check the state of the certificates for the Zenith services: On the K3S node, targetting the HA cluster if deployed $ kubectl -n zenith-services get certificate NAME READY SECRET AGE tls-cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g True tls-cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g 2d tls-i03xvflgk1zmtcsdm2x5z5lz9qz05027euw True tls-i03xvflgk1zmtcsdm2x5z5lz9qz05027euw 2d tls-txn3zidfdnru5rg109voh848n51rvicmr1s True tls-txn3zidfdnru5rg109voh848n51rvicmr1s 2d If the certificate for the service is not ready, check the details for the certificate using kubectl describe and check for any errors that have occured.","title":"Debugging Zenith services"},{"location":"debugging/zenith-services/#debugging-zenith-services","text":"If a Zenith service does not become available, the most common causes are:","title":"Debugging Zenith services"},{"location":"debugging/zenith-services/#client-not-registering-correctly-with-sshd","text":"To determine if this is the case, it is useful to access the Consul UI. Similar to the monitoring, this interface is only accessible inside the cluster. To access it, use the following command: ./bin/port-forward consul 3000 The Consul UI will then be available at http://localhost:3000 . The default view shows Consul's view of the services, where you can check if the service is being registered correctly. If the client is not correctly registering with SSHD, the first thing to do is check the logs for the client and restart it if necessary. If the client is failing to connect to SSHD, then try restarting SSHD: On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sshd","title":"Client not registering correctly with SSHD"},{"location":"debugging/zenith-services/#kubernetes-resources-for-the-zenith-service-have-not-been-created","text":"If the service exists in Consul, it is possible that the process that synchronises Consul services with Kubernetes resources is not functioning correctly. To check if Kubernetes resources are being created, run the following command and check that the Ingress , Service and Endpoints resources have been created for the service: On the K3S node, targetting the HA cluster if deployed $ kubectl -n zenith-services get ingress,service,endpoints NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g nginx cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g.apps.example.org 96.241.100.96 80, 443 2d ingress.networking.k8s.io/i03xvflgk1zmtcsdm2x5z5lz9qz05027euw nginx i03xvflgk1zmtcsdm2x5z5lz9qz05027euw.apps.example.org 96.241.100.96 80, 443 2d ingress.networking.k8s.io/pxmvy7235x2ggfvf2op615gvz2v59wkqglc nginx pxmvy7235x2ggfvf2op615gvz2v59wkqglc.apps.example.org 96.241.100.96 80, 443 2d ingress.networking.k8s.io/txn3zidfdnru5rg109voh848n51rvicmr1s nginx txn3zidfdnru5rg109voh848n51rvicmr1s.apps.example.org 96.241.100.96 80, 443 2d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g ClusterIP 172.27.10.109 <none> 80/TCP 2d service/i03xvflgk1zmtcsdm2x5z5lz9qz05027euw ClusterIP 172.30.203.227 <none> 80/TCP 2d service/pxmvy7235x2ggfvf2op615gvz2v59wkqglc ClusterIP 172.28.51.148 <none> 80/TCP 2d service/txn3zidfdnru5rg109voh848n51rvicmr1s ClusterIP 172.29.136.245 <none> 80/TCP 2d NAME ENDPOINTS AGE endpoints/cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g 172.18.152.99:34665 2d endpoints/i03xvflgk1zmtcsdm2x5z5lz9qz05027euw 172.18.152.99:39409 2d endpoints/pxmvy7235x2ggfvf2op615gvz2v59wkqglc 172.18.152.99:44761,172.18.152.99:36483,172.18.152.99:46449 2d endpoints/txn3zidfdnru5rg109voh848n51rvicmr1s 172.18.152.99:45379 2d Tip If an ingress resource does not have an IP, this may be a sign that the ingress controller is not correctly configured or not functioning correctly. If they do not exist, try restarting the Zenith sync component: On the K3S node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sync","title":"Kubernetes resources for the Zenith service have not been created"},{"location":"debugging/zenith-services/#cert-manager-fails-to-obtain-a-certificate","text":"If you are using cert-manager to dynamically allocate certificates for Zenith services it is possible that cert-manager has failed to obtain a certificate for the service, e.g. because it has been rate-limited. To check if this is the case, check the state of the certificates for the Zenith services: On the K3S node, targetting the HA cluster if deployed $ kubectl -n zenith-services get certificate NAME READY SECRET AGE tls-cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g True tls-cjzm03yczuj6oqrj3h8htl4u1bbx96qd53g 2d tls-i03xvflgk1zmtcsdm2x5z5lz9qz05027euw True tls-i03xvflgk1zmtcsdm2x5z5lz9qz05027euw 2d tls-txn3zidfdnru5rg109voh848n51rvicmr1s True tls-txn3zidfdnru5rg109voh848n51rvicmr1s 2d If the certificate for the service is not ready, check the details for the certificate using kubectl describe and check for any errors that have occured.","title":"cert-manager fails to obtain a certificate"},{"location":"deployment/","text":"Deploying Azimuth Python dependencies The Python requirements for an Azimuth deployment host, including Ansible itself, are contained in requirements.txt and must be installed before you can proceed with a deployment. It is recommended to use a virtual environment in order to keep the dependencies isolated from other Python applications on the host. azimuth-config includes a utility script that will create a Python virtual environment in the configuration directory and install the required dependencies: ./bin/ensure-venv If it exists, this virtual environment will be activated as part of the environment activation (see below). If you prefer to manage your own virtual environments then you must ensure that the correct environment is activated and has the required dependencies installed before continuing. For example, if you use pyenv you can set the PYENV_VERSION environment variable in your azimuth-config environment : env PYENV_VERSION = azimuth-config Activating an environment Before you can deploy Azimuth, you must first activate an environment: source ./bin/activate my-site Warning This script must be source d rather than just executed as it exports environment variables into the current shell that are used to configure the deployment. Deploying an environment Once you are happy with any configuration changes and the environment that you want to deploy to has been activated, run the following command to deploy Azimuth: # Install or update Ansible dependencies ansible-galaxy install -f -r ./requirements.yml # Run the provision playbook from the azimuth-ops collection # The inventory is picked up from the ansible.cfg file in the environment ansible-playbook stackhpc.azimuth_ops.provision Tearing down an environment azimuth-ops is also able to tear down an Azimuth environment, including the K3S and HA Kubernetes clusters as required. After activating the environment that you want to tear down, run the following: ansible-playbook stackhpc.azimuth_ops.destroy","title":"Deploying Azimuth"},{"location":"deployment/#deploying-azimuth","text":"","title":"Deploying Azimuth"},{"location":"deployment/#python-dependencies","text":"The Python requirements for an Azimuth deployment host, including Ansible itself, are contained in requirements.txt and must be installed before you can proceed with a deployment. It is recommended to use a virtual environment in order to keep the dependencies isolated from other Python applications on the host. azimuth-config includes a utility script that will create a Python virtual environment in the configuration directory and install the required dependencies: ./bin/ensure-venv If it exists, this virtual environment will be activated as part of the environment activation (see below). If you prefer to manage your own virtual environments then you must ensure that the correct environment is activated and has the required dependencies installed before continuing. For example, if you use pyenv you can set the PYENV_VERSION environment variable in your azimuth-config environment : env PYENV_VERSION = azimuth-config","title":"Python dependencies"},{"location":"deployment/#activating-an-environment","text":"Before you can deploy Azimuth, you must first activate an environment: source ./bin/activate my-site Warning This script must be source d rather than just executed as it exports environment variables into the current shell that are used to configure the deployment.","title":"Activating an environment"},{"location":"deployment/#deploying-an-environment","text":"Once you are happy with any configuration changes and the environment that you want to deploy to has been activated, run the following command to deploy Azimuth: # Install or update Ansible dependencies ansible-galaxy install -f -r ./requirements.yml # Run the provision playbook from the azimuth-ops collection # The inventory is picked up from the ansible.cfg file in the environment ansible-playbook stackhpc.azimuth_ops.provision","title":"Deploying an environment"},{"location":"deployment/#tearing-down-an-environment","text":"azimuth-ops is also able to tear down an Azimuth environment, including the K3S and HA Kubernetes clusters as required. After activating the environment that you want to tear down, run the following: ansible-playbook stackhpc.azimuth_ops.destroy","title":"Tearing down an environment"},{"location":"deployment/automation/","text":"Automating deployments For a production installation of Azimuth, it is recommended to adopt a continuous delivery approach to deployment rather than running deployment commands manually. Using this approach, configuration changes are automatically deployed to test, staging and production environments, although deployments to production typically include a manual approval. Continuous delivery vs continuous deployment Continuous delivery is very similar to Continuous Deployment with the exception that in continuous deployment, production deployments are also fully automated with no manual intervention or approval. Using a site mixin To get the maximum benefit from automated deployments and the feature branch workflow , you should try to minimise the differences between the production, staging and dynamic review environments. The best way to do this is to use a site mixin that contains all the site-specific configuration that is common between your environments, e.g. extra community images, custom Kubernetes templates, networking configuration, and include it in each of your concrete environments. GitLab CI/CD azimuth-config provides a sample configuration for use with GitLab CI/CD that demonstrates how to set up continuous delivery for an Azimuth configuration repository. Tip If you are using GitLab for your configuration repository, make sure you have configured it to use GitLab-managed Terraform state . Runner configuration Configuration of GitLab runners for executing CI/CD jobs is beyond the scope of this documentation. We assume that a runner is available to the configuration project that is able to execute user-specified images, e.g. using the Docker or Kubernetes executors. One option is to deploy a runner as a VM in an OpenStack project . Automated deployments The sample GitLab CI/CD configuration makes use of GitLab environments to manage deployments of Azimuth, where each GitLab environment uses a concrete configuration environment in your repository. This is a one-to-one relationship except for per-branch dynamic review environments , where multiple GitLab environments will use a single configuration environment. If you are using GitLab-managed Terraform state, each GitLab environment (not configuration environment) will get it's own independent Terraform state. The sample configuration defines the following deployment jobs: Each commit to a branch other than main (e.g. a feature branch), triggers an automated deployment to a branch-specific dynamic GitLab environment , using a single concrete configuration environment . These environments are automatically destroyed when the associated merge request is closed. Each commit to main triggers an automated deployment to staging using a static GitLab environment . Each commit to main also creates a job for an automated deployment to production, also using a static environment. However this job requires a manual trigger before it will start. To get started, just copy .gitlab-ci.yml.sample to .gitlab-ci.yml and amend the environment names and paths to match the environments in your configuration. The next commit will begin to trigger deployments. Access to secrets In order for the deployment jobs to access the secrets in your configuration, you will need to provide the git-crypt key as a CI/CD variable for the project . The base64-encoded key should be stored in the GIT_CRYPT_KEY_B64 variable and made available to all branches: git-crypt export-key - | base64 Per-branch dynamic review environments The per-branch dynamic review environments are special in that multiple GitLab environments are provisioned using a single configuration environment. This means that the configuration environment must be capable of producing multiple independent deployments. In particular, it cannot use any fixed floating IPs which also means no fixed DNS entry. Instead it must allocate an IP for itself and use a dynamic DNS service like sslip.io for the ingress domain. This is also how the demo environment works, and is the default if no fixed IP is specified. Single node only At present dynamic review environments must be single node deployments , as HA deployments do not support dynamically allocating a floating IP for the Ingress Controller. A single node is likely to be sufficient for a dynamic review environment, as a full HA deployment for every branch would consume a lot more resources. However you should ensure that you have a full HA deployment as a staging or pre-production environment in order to test that the configuration works. Shared credentials Per-branch dynamic review environments will share the clouds.yaml specified in the configuration environment, and hence will share an OpenStack project. This is not a problem, as multiple isolated deployments can happily coexist in the same project as long as they have different names, but you must ensure that the project has suitable quotas. Activating per-branch review environments Because a single configuration environment is used for multiple deployments, a slight variant of the usual environment activation must be used that specifies both the configuration environment and the GitLab environment name: source ./bin/activate \"<configuration environment>\" \"<gitlab environment>\" A configuration environment for dynamic review environments is set up in the usual way , subject to the caveats above. The following is a minimal set of Ansible variables that will work for most clouds, when combined with the base and singlenode mixin environments (plus any site-specific mixin environments): # Configuration for the K3S node infra_external_network_id : \"<network id>\" infra_flavor_id : \"<flavor id>\" # CaaS configuration azimuth_caas_stackhpc_slurm_appliance_login_flavor_name : \"<flavor name>\" azimuth_caas_stackhpc_slurm_appliance_control_flavor_name : \"<flavor name>\" # Azimuth cloud name # This can use the environment name if desired, e.g.: azimuth_current_cloud_name : \"{{ lookup('env', 'CI_ENVIRONMENT_SLUG') }}\" azimuth_current_cloud_label : \"{{ lookup('env', 'CI_ENVIRONMENT_NAME') }}\" # \"Secrets\" # Since the dynamic environments are short-lived, there is not much # risk in using secrets that are not really secret for ease harbor_admin_password : admin harbor_secret_key : notsecret0123456 awx_admin_password : admin zenith_registrar_subdomain_token_signing_key : notsecret azimuth_secret_key : notsecret Automated synchronisation of upstream changes The sample configuration also includes a job that can automatically synchronise changes from upstream . If the job detects changes, it will create a new branch, merge the changes into it and create an associated merge request . If you also have per-branch dynamic review environments enabled, then this will automatically trigger a job to deploy the changes for review. The job will only run for a scheduled pipeline , so if you want to have automatic synchronisation of upstream changes you must add a pipeline schedule for the main branch of your configuration repository with a suitable interval (e.g. daily or weekly). Because the job needs to write to the repository and call the merge requests API, the CI/CD job token is not sufficient. Instead, you must set the CI/CD variables GITLAB_PAT_TOKEN and GITLAB_PAT_USERNAME for the scheduled pipeline, which should contain an access token and the corresponding username respectively. The token must have permission to write to the repository and the GitLab API. If your GitLab project has Project access tokens available, then one of these can be used by specifying the associated bot user . Unfortunately, this is a paid feature and the only real alternative is to use a Personal access token . Danger Personal access tokens cannot be scoped to a project. Uploading a personal access token as a CI/CD variable means that other members of the project, and the CI/CD jobs that use it, will be able to see your token. Because of the lack of project scope, this means that a malicious actor may be able to obtain the token and use it to access your other projects. If you do not want to pay for Project access tokens, then you could register a separate service account that only belongs to your configuration project and issue a personal access token from that account instead.","title":"Automating deployments"},{"location":"deployment/automation/#automating-deployments","text":"For a production installation of Azimuth, it is recommended to adopt a continuous delivery approach to deployment rather than running deployment commands manually. Using this approach, configuration changes are automatically deployed to test, staging and production environments, although deployments to production typically include a manual approval. Continuous delivery vs continuous deployment Continuous delivery is very similar to Continuous Deployment with the exception that in continuous deployment, production deployments are also fully automated with no manual intervention or approval. Using a site mixin To get the maximum benefit from automated deployments and the feature branch workflow , you should try to minimise the differences between the production, staging and dynamic review environments. The best way to do this is to use a site mixin that contains all the site-specific configuration that is common between your environments, e.g. extra community images, custom Kubernetes templates, networking configuration, and include it in each of your concrete environments.","title":"Automating deployments"},{"location":"deployment/automation/#gitlab-cicd","text":"azimuth-config provides a sample configuration for use with GitLab CI/CD that demonstrates how to set up continuous delivery for an Azimuth configuration repository. Tip If you are using GitLab for your configuration repository, make sure you have configured it to use GitLab-managed Terraform state . Runner configuration Configuration of GitLab runners for executing CI/CD jobs is beyond the scope of this documentation. We assume that a runner is available to the configuration project that is able to execute user-specified images, e.g. using the Docker or Kubernetes executors. One option is to deploy a runner as a VM in an OpenStack project .","title":"GitLab CI/CD"},{"location":"deployment/automation/#automated-deployments","text":"The sample GitLab CI/CD configuration makes use of GitLab environments to manage deployments of Azimuth, where each GitLab environment uses a concrete configuration environment in your repository. This is a one-to-one relationship except for per-branch dynamic review environments , where multiple GitLab environments will use a single configuration environment. If you are using GitLab-managed Terraform state, each GitLab environment (not configuration environment) will get it's own independent Terraform state. The sample configuration defines the following deployment jobs: Each commit to a branch other than main (e.g. a feature branch), triggers an automated deployment to a branch-specific dynamic GitLab environment , using a single concrete configuration environment . These environments are automatically destroyed when the associated merge request is closed. Each commit to main triggers an automated deployment to staging using a static GitLab environment . Each commit to main also creates a job for an automated deployment to production, also using a static environment. However this job requires a manual trigger before it will start. To get started, just copy .gitlab-ci.yml.sample to .gitlab-ci.yml and amend the environment names and paths to match the environments in your configuration. The next commit will begin to trigger deployments.","title":"Automated deployments"},{"location":"deployment/automation/#access-to-secrets","text":"In order for the deployment jobs to access the secrets in your configuration, you will need to provide the git-crypt key as a CI/CD variable for the project . The base64-encoded key should be stored in the GIT_CRYPT_KEY_B64 variable and made available to all branches: git-crypt export-key - | base64","title":"Access to secrets"},{"location":"deployment/automation/#per-branch-dynamic-review-environments","text":"The per-branch dynamic review environments are special in that multiple GitLab environments are provisioned using a single configuration environment. This means that the configuration environment must be capable of producing multiple independent deployments. In particular, it cannot use any fixed floating IPs which also means no fixed DNS entry. Instead it must allocate an IP for itself and use a dynamic DNS service like sslip.io for the ingress domain. This is also how the demo environment works, and is the default if no fixed IP is specified. Single node only At present dynamic review environments must be single node deployments , as HA deployments do not support dynamically allocating a floating IP for the Ingress Controller. A single node is likely to be sufficient for a dynamic review environment, as a full HA deployment for every branch would consume a lot more resources. However you should ensure that you have a full HA deployment as a staging or pre-production environment in order to test that the configuration works. Shared credentials Per-branch dynamic review environments will share the clouds.yaml specified in the configuration environment, and hence will share an OpenStack project. This is not a problem, as multiple isolated deployments can happily coexist in the same project as long as they have different names, but you must ensure that the project has suitable quotas. Activating per-branch review environments Because a single configuration environment is used for multiple deployments, a slight variant of the usual environment activation must be used that specifies both the configuration environment and the GitLab environment name: source ./bin/activate \"<configuration environment>\" \"<gitlab environment>\" A configuration environment for dynamic review environments is set up in the usual way , subject to the caveats above. The following is a minimal set of Ansible variables that will work for most clouds, when combined with the base and singlenode mixin environments (plus any site-specific mixin environments): # Configuration for the K3S node infra_external_network_id : \"<network id>\" infra_flavor_id : \"<flavor id>\" # CaaS configuration azimuth_caas_stackhpc_slurm_appliance_login_flavor_name : \"<flavor name>\" azimuth_caas_stackhpc_slurm_appliance_control_flavor_name : \"<flavor name>\" # Azimuth cloud name # This can use the environment name if desired, e.g.: azimuth_current_cloud_name : \"{{ lookup('env', 'CI_ENVIRONMENT_SLUG') }}\" azimuth_current_cloud_label : \"{{ lookup('env', 'CI_ENVIRONMENT_NAME') }}\" # \"Secrets\" # Since the dynamic environments are short-lived, there is not much # risk in using secrets that are not really secret for ease harbor_admin_password : admin harbor_secret_key : notsecret0123456 awx_admin_password : admin zenith_registrar_subdomain_token_signing_key : notsecret azimuth_secret_key : notsecret","title":"Per-branch dynamic review environments"},{"location":"deployment/automation/#automated-synchronisation-of-upstream-changes","text":"The sample configuration also includes a job that can automatically synchronise changes from upstream . If the job detects changes, it will create a new branch, merge the changes into it and create an associated merge request . If you also have per-branch dynamic review environments enabled, then this will automatically trigger a job to deploy the changes for review. The job will only run for a scheduled pipeline , so if you want to have automatic synchronisation of upstream changes you must add a pipeline schedule for the main branch of your configuration repository with a suitable interval (e.g. daily or weekly). Because the job needs to write to the repository and call the merge requests API, the CI/CD job token is not sufficient. Instead, you must set the CI/CD variables GITLAB_PAT_TOKEN and GITLAB_PAT_USERNAME for the scheduled pipeline, which should contain an access token and the corresponding username respectively. The token must have permission to write to the repository and the GitLab API. If your GitLab project has Project access tokens available, then one of these can be used by specifying the associated bot user . Unfortunately, this is a paid feature and the only real alternative is to use a Personal access token . Danger Personal access tokens cannot be scoped to a project. Uploading a personal access token as a CI/CD variable means that other members of the project, and the CI/CD jobs that use it, will be able to see your token. Because of the lack of project scope, this means that a malicious actor may be able to obtain the token and use it to access your other projects. If you do not want to pay for Project access tokens, then you could register a separate service account that only belongs to your configuration project and issue a personal access token from that account instead.","title":"Automated synchronisation of upstream changes"},{"location":"repository/","text":"Azimuth configuration repository The azimuth-config repository provides best-practice configuration for Azimuth deployments that can be inherited by site-specific configuration repositories using Git . Using Git makes it easy to periodically incorporate changes to the best practice into the configuration for your site, e.g. to pick up new Azimuth versions, updated images, CaaS appliance versions or Kubernetes versions. Initial repository setup First make an empty Git repository using your service of choice (e.g. GitHub or GitLab ), then execute the following commands to turn the new empty repository into a copy of the azimuth-config repository: # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config.git my-azimuth-config cd my-azimuth-config # Maintain the existing origin remote so that we can periodically sync changes, # but rename it to upstream git remote rename origin upstream # Create a new origin remote for the new repository location git remote add origin git@<repo location>/my-azimuth-config.git # Push the main branch to the new origin git push -u origin main You now have an independent copy of the azimuth-config repository that has a link back to the source repository via the upstream remote. Creating a new environment Your new repository does not yet contain any site-specific configuration. The best way to do this is to copy the example environment as a starting point: cp -r ./environments/example ./environments/my-site Tip Copying the example environment, rather than just renaming it, avoids conflicts when synchronising changes from the azimuth-config repository where the example environment has changed. Once you have your new environment, you can make the required changes for your site. As you make changes to your environment, remember to commit and push them regularly: git add ./environments/my-site git commit -m \"Made some changes to my environment\" git push Making changes to your environment Once you have an environment deployed, it is recommended to use a feature branch workflow when making changes to your configuration repository. Automated deployments The feature branch workflow works particularly well when you use a continuous delivery approach to automate deployments . In this workflow, the required changes are made on a branch in the configuration repository. Once you are happy with the changes, you create a merge (or pull) request proposing the changes to main . These changes can then be reviewed before being merged to main . If you have automated deployments, the branch may even get a dynamic environment created for it where the result of the changes can be verified before the merge takes place. Synchronising changes from upstream Over time, as Azimuth changes, the best-practice configuration will also change to point at new Azimuth versions, upgraded dependencies and new images. Tip This process can be automated if you have the tooling available. To incorporate the latest changes into your site-specific repository, use the following: git fetch upstream git merge upstream/main At this point, you will need to fix any conflicts where you have made changes to the same files that have been changed by azimuth-config . Avoiding conflicts To avoid conflicts, you should never directly modify any files that come from azimuth-config - instead you should use the environment layering to override variables where required, and copy files if necessary. Once any conflicts have been resolved, you can commit and push the changes: git commit -m \"Merge changes from upstream\" git push","title":"Azimuth configuration repository"},{"location":"repository/#azimuth-configuration-repository","text":"The azimuth-config repository provides best-practice configuration for Azimuth deployments that can be inherited by site-specific configuration repositories using Git . Using Git makes it easy to periodically incorporate changes to the best practice into the configuration for your site, e.g. to pick up new Azimuth versions, updated images, CaaS appliance versions or Kubernetes versions.","title":"Azimuth configuration repository"},{"location":"repository/#initial-repository-setup","text":"First make an empty Git repository using your service of choice (e.g. GitHub or GitLab ), then execute the following commands to turn the new empty repository into a copy of the azimuth-config repository: # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config.git my-azimuth-config cd my-azimuth-config # Maintain the existing origin remote so that we can periodically sync changes, # but rename it to upstream git remote rename origin upstream # Create a new origin remote for the new repository location git remote add origin git@<repo location>/my-azimuth-config.git # Push the main branch to the new origin git push -u origin main You now have an independent copy of the azimuth-config repository that has a link back to the source repository via the upstream remote.","title":"Initial repository setup"},{"location":"repository/#creating-a-new-environment","text":"Your new repository does not yet contain any site-specific configuration. The best way to do this is to copy the example environment as a starting point: cp -r ./environments/example ./environments/my-site Tip Copying the example environment, rather than just renaming it, avoids conflicts when synchronising changes from the azimuth-config repository where the example environment has changed. Once you have your new environment, you can make the required changes for your site. As you make changes to your environment, remember to commit and push them regularly: git add ./environments/my-site git commit -m \"Made some changes to my environment\" git push","title":"Creating a new environment"},{"location":"repository/#making-changes-to-your-environment","text":"Once you have an environment deployed, it is recommended to use a feature branch workflow when making changes to your configuration repository. Automated deployments The feature branch workflow works particularly well when you use a continuous delivery approach to automate deployments . In this workflow, the required changes are made on a branch in the configuration repository. Once you are happy with the changes, you create a merge (or pull) request proposing the changes to main . These changes can then be reviewed before being merged to main . If you have automated deployments, the branch may even get a dynamic environment created for it where the result of the changes can be verified before the merge takes place.","title":"Making changes to your environment"},{"location":"repository/#synchronising-changes-from-upstream","text":"Over time, as Azimuth changes, the best-practice configuration will also change to point at new Azimuth versions, upgraded dependencies and new images. Tip This process can be automated if you have the tooling available. To incorporate the latest changes into your site-specific repository, use the following: git fetch upstream git merge upstream/main At this point, you will need to fix any conflicts where you have made changes to the same files that have been changed by azimuth-config . Avoiding conflicts To avoid conflicts, you should never directly modify any files that come from azimuth-config - instead you should use the environment layering to override variables where required, and copy files if necessary. Once any conflicts have been resolved, you can commit and push the changes: git commit -m \"Merge changes from upstream\" git push","title":"Synchronising changes from upstream"},{"location":"repository/secrets/","text":"Managing secrets Each Azimuth environment contains a small number of files and variable values that must be kept secret. However we would also like to have these files and variables in version control so that we can track when they have changed and share them with others in our team. The way to achieve both these goals is to encrypt the secrets when at rest in the Git repository. For an Azimuth configuration repository, the recommended method for doing this is using git-crypt . git-crypt provides transparent encryption and decryption of files in a Git repository using Git filters . This allows for a mix of encrypted and unencrypted content in the same repository, which is exactly what we want for an Azimuth configuration. By encrypting files when they are committed and decrypting them when they are checked out, git-crypt allows you to work as if the encryption is not present while being confident that your secrets remain private. Team members can also work on the public parts of the repository without decrypting the private parts if you wish to maintain separation of privilege. git-crypt works on entire files, not at the variable level, so it is recommended that encrypted files contain only secret values so that information is not hidden unnecessarily. To this end, a typical azimuth-config environment for a site will have one or more group_vars files in the inventory that are unencrypted, but secret values will be placed in a secrets.yaml that is encrypted. Initialising git-crypt To initialise git-crypt for your config repository, first make sure that the CLI is installed. This can be installed using the package manager on most major Linux operating systems, using Homebrew on Mac OSX or by building from source . Then execute the following command to begin encrypting files: git-crypt init Danger If you lose access to the key that git-crypt generates to encrypt your repository, you will be locked out of the repository for good. It is recommended that you export the key (see below) and store it somewhere safe, e.g. in your organisation's secret store. Verifying which files are encrypted azimuth-config contains a .gitattributes file that ensures all clouds.yaml , secrets.yaml , env.secret and TLS key files in the repository will be encrypted (except those for the example environment). If required, you can add additional patterns to the file to encrypt other files. You can check the files that git-crypt is encrypting using the following command: git-crypt status -e Granting access to others As mentioned above, team members can work on the public parts of the repository without decrypting the encrypted files. However in order to make a deployment, the secrets must be decrypted - git-crypt refers to this process as \"unlocking\". Using a shared secret key The simplest way to grant access to the encrypted files in a repository is by sharing the secret key generated by git-crypt . The key is binary, so the best way to share the key is by base64-encoding it: git-crypt export-key - | base64 To unlock the repository you must first clone it, then use the key to unlock it: # Clone the repository git clone $REPOSITORY_URL # Move into the repository directory cd $REPOSITORY_DIR # Unlock it using the base64-encoded key echo $GIT_CRYPT_KEY_B64 | base64 -d | git-crypt unlock - Using GPG keys git-crypt is also able to grant access to the repository using GPG keys . This avoids the use of shared secrets and makes it explicit in the repository who has access. A full discussion of the use of GPG is beyond the scope of this documentation, including the generation of keys, as it differs substantially depending on your operating system. To add a GPG key to your repository, use the following command: git-crypt add-gpg-user USER_ID where USER_ID is a key ID, a full fingerprint, an email address, or anything else that uniquely identifies a public key to GPG. You will then need to push the changes to the repository that encode the user's access: git push To unlock a repository using your GPG identity, just execute: git-crypt unlock","title":"Managing secrets"},{"location":"repository/secrets/#managing-secrets","text":"Each Azimuth environment contains a small number of files and variable values that must be kept secret. However we would also like to have these files and variables in version control so that we can track when they have changed and share them with others in our team. The way to achieve both these goals is to encrypt the secrets when at rest in the Git repository. For an Azimuth configuration repository, the recommended method for doing this is using git-crypt . git-crypt provides transparent encryption and decryption of files in a Git repository using Git filters . This allows for a mix of encrypted and unencrypted content in the same repository, which is exactly what we want for an Azimuth configuration. By encrypting files when they are committed and decrypting them when they are checked out, git-crypt allows you to work as if the encryption is not present while being confident that your secrets remain private. Team members can also work on the public parts of the repository without decrypting the private parts if you wish to maintain separation of privilege. git-crypt works on entire files, not at the variable level, so it is recommended that encrypted files contain only secret values so that information is not hidden unnecessarily. To this end, a typical azimuth-config environment for a site will have one or more group_vars files in the inventory that are unencrypted, but secret values will be placed in a secrets.yaml that is encrypted.","title":"Managing secrets"},{"location":"repository/secrets/#initialising-git-crypt","text":"To initialise git-crypt for your config repository, first make sure that the CLI is installed. This can be installed using the package manager on most major Linux operating systems, using Homebrew on Mac OSX or by building from source . Then execute the following command to begin encrypting files: git-crypt init Danger If you lose access to the key that git-crypt generates to encrypt your repository, you will be locked out of the repository for good. It is recommended that you export the key (see below) and store it somewhere safe, e.g. in your organisation's secret store.","title":"Initialising git-crypt"},{"location":"repository/secrets/#verifying-which-files-are-encrypted","text":"azimuth-config contains a .gitattributes file that ensures all clouds.yaml , secrets.yaml , env.secret and TLS key files in the repository will be encrypted (except those for the example environment). If required, you can add additional patterns to the file to encrypt other files. You can check the files that git-crypt is encrypting using the following command: git-crypt status -e","title":"Verifying which files are encrypted"},{"location":"repository/secrets/#granting-access-to-others","text":"As mentioned above, team members can work on the public parts of the repository without decrypting the encrypted files. However in order to make a deployment, the secrets must be decrypted - git-crypt refers to this process as \"unlocking\".","title":"Granting access to others"},{"location":"repository/secrets/#using-a-shared-secret-key","text":"The simplest way to grant access to the encrypted files in a repository is by sharing the secret key generated by git-crypt . The key is binary, so the best way to share the key is by base64-encoding it: git-crypt export-key - | base64 To unlock the repository you must first clone it, then use the key to unlock it: # Clone the repository git clone $REPOSITORY_URL # Move into the repository directory cd $REPOSITORY_DIR # Unlock it using the base64-encoded key echo $GIT_CRYPT_KEY_B64 | base64 -d | git-crypt unlock -","title":"Using a shared secret key"},{"location":"repository/secrets/#using-gpg-keys","text":"git-crypt is also able to grant access to the repository using GPG keys . This avoids the use of shared secrets and makes it explicit in the repository who has access. A full discussion of the use of GPG is beyond the scope of this documentation, including the generation of keys, as it differs substantially depending on your operating system. To add a GPG key to your repository, use the following command: git-crypt add-gpg-user USER_ID where USER_ID is a key ID, a full fingerprint, an email address, or anything else that uniquely identifies a public key to GPG. You will then need to push the changes to the repository that encode the user's access: git push To unlock a repository using your GPG identity, just execute: git-crypt unlock","title":"Using GPG keys"},{"location":"repository/terraform/","text":"Terraform state azimuth-ops uses Terraform to manage the K3S node in both the single node and high-availability deployment methods. In order to keep track of the resources that it has created, and how they map to the resources in the Terraform configuration generated by azimuth-ops , Terraform must store its state somewhere. The location of the state is determined by the backend configuration . Each environment in an azimuth-config repository has a corresponding Terraform state, and they are independent from each other. Local state By default azimuth-ops will use the local backend, which stores the Terraform state as a file on the local disk in the .work directory. This requires no explicit configuration, but comes with the usual caveats about keeping important state on your local machine. Not suitable for production Local state is sufficient for a demonstration or evaluation, but for a shared or production deployment it is recommended to use remote state. Remote state Terraform supports a number of remote backends that can be used to persist Terraform state independently of where a deployment is run. This allows deployments to be made from anywhere that can access the state without corrupting or conflicting with any existing resources from previous deployments. Tip If you want to use the same remote backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. Warning In order to avoid multiple writers when using remote state, it is recommended to use a backend that supports state locking . Secret configuration Some of the configuration variables for remote backends, e.g. passwords and keys, should be kept secret. If you want to keep such variables in Git - which is recommended where possible - then they must be encrypted . HTTP The HTTP backend is a simple REST client that uses GET , POST and DELETE HTTP requests to manage a Terraform state. State locking is optional, but it is recommended to use an implementation that supports it. To use the HTTP backend, you must set at least the following variables: environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : http # The state endpoint for the environment # # Using the azimuth_environment variable in the address means that each # concrete environment gets different state URL even if this configuration # is in a shared mixin environment terraform_backend_config : address : \"https://example.org/tfstate/{{ azimuth_environment }}\" For the full set of available config options, see the Terraform docs for the HTTP backend . GitLab Tip This is the recommended option if you are using GitLab for your config repository. If you are using GitLab to host your configuration repository, either gitlab.com or self-hosted, you can use GitLab-managed Terraform state to store the Terraform state for your environments. GitLab provides a HTTP backend that can be configured as follows: environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : http # The API base URL for the target GitLab project # For a self-hosted GitLab instance, replace gitlab.com with your domain gitlab_project_url : \"https://gitlab.com/api/v4/projects/<project id>\" # The state endpoint for the environment # # Using the azimuth_environment variable as the state name means that each # concrete environment gets a separate managed Terraform state even if this # configuration is in a shared mixin environment terraform_http_address : \"{{ gitlab_project_url }}/terraform/state/{{ azimuth_environment }}\" # The state-locking and unlocking endpoints for the environment terraform_http_lock_address : \"{{ terraform_http_address }}/lock\" terraform_http_lock_method : POST terraform_http_unlock_address : \"{{ terraform_http_lock_address }}\" terraform_http_unlock_method : DELETE terraform_backend_config : address : \"{{ terraform_http_address }}\" lock_address : \"{{ terraform_http_lock_address }}\" lock_method : \"{{ terraform_http_lock_method }}\" unlock_address : \"{{ terraform_http_unlock_address }}\" unlock_method : \"{{ terraform_http_unlock_method }}\" Tip If you want to use the same backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. The username and password (or token) that are used to authenticate with GitLab to manage the Terraform state are set using the TF_HTTP_USERNAME and TF_HTTP_PASSWORD environment variables respectively. If you are using GitLab CI/CD to automate deployments , then the pipeline will be issued with a suitable token. The sample configuration includes configuration to populate these variables using this token. If you are not using automation but your GitLab installation has project access tokens available, you can configure a project access token and store it (encrypted!) in the env.secret file, referencing the bot username: env.secret TF_HTTP_USERNAME = \"project_<id>_bot\" TF_HTTP_PASSWORD = \"<project access token>\" If you need to access an environment deployed using automation, or you do not have project access tokens available, then you can use a Personal access token , which at least avoids using your password. Never commit personal access tokens You should never commit a personal access token to the configuration repository, even encrypted, because it is not possible to set a project scope. If using a personal access token, you should export the relevant variables before activating an environment: export TF_HTTP_USERNAME = \"<username>\" export TF_HTTP_PASSWORD = \"<token>\" source ./bin/activate my-site S3 Terraform also has an S3 backend that is able to store state in any S3-compatible object store, such as Amazon S3 or Ceph Object Gateway . Depending on the provider of your object store, the specific configuration options in the following section may differ. The example configuration shown here is for a Ceph object store as Ceph is often used together with OpenStack. See the Terraform docs for the S3 backend for all the available options. environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : s3 # The endpoint of the object store terraform_s3_endpoint : object.example.com # The region to use # Ceph does not normally use the region, but Terraform requires it terraform_s3_region : not-used-but-required terraform_s3_skip_region_validation : \"true\" # The bucket to put Terraform states in # NOTE: This bucket must already exist - it will not be created by Terraform terraform_s3_bucket : azimuth-terraform-states # The key to use for the state for the environment # # Using the azimuth_environment variable in the key means that the state # for each concrete environment is stored in a separate key, even if this # configuration is in a shared mixin environment terraform_s3_key : \"{{ azimuth_environment }}.tfstate\" # The STS API doesn't exist for Ceph terraform_s3_skip_credentials_validation : \"true\" # Tell Terraform to use path-style URLs, e.g. <host>/<bucket>, instead of # subdomain-style URLs, e.g. <bucket>.<host> terraform_s3_force_path_style : \"true\" terraform_backend_config : endpoint : \"{{ terraform_s3_endpoint }}\" region : \"{{ terraform_s3_region }}\" bucket : \"{{ terraform_s3_bucket }}\" key : \"{{ terraform_s3_key }}\" skip_credentials_validation : \"{{ terraform_s3_skip_credentials_validation }}\" force_path_style : \"{{ terraform_s3_force_path_style }}\" skip_region_validation : \"{{ terraform_s3_skip_region_validation }}\" The S3 credentials (access key ID and secret), can be specified either as environment variables or as Ansible variables. To use environment variables, just place the credentials in env.secret : env.secret AWS_ACCESS_KEY_ID = \"<access key id>\" AWS_SECRET_ACCESS_KEY = \"<secret key>\" To use Ansible variables, they should be added to secrets.yml and referenced in the terraform_backend_config variable: environments/my-site/inventory/group_vars/all/secrets.yml terraform_s3_access_key : \"<access key id>\" terraform_s3_secret_key : \"<secret key>\" environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_config : # ... other options ... access_key : \"{{ terraform_s3_access_key }}\" secret_key : \"{{ terraform_s3_secret_key }}\"","title":"Terraform state"},{"location":"repository/terraform/#terraform-state","text":"azimuth-ops uses Terraform to manage the K3S node in both the single node and high-availability deployment methods. In order to keep track of the resources that it has created, and how they map to the resources in the Terraform configuration generated by azimuth-ops , Terraform must store its state somewhere. The location of the state is determined by the backend configuration . Each environment in an azimuth-config repository has a corresponding Terraform state, and they are independent from each other.","title":"Terraform state"},{"location":"repository/terraform/#local-state","text":"By default azimuth-ops will use the local backend, which stores the Terraform state as a file on the local disk in the .work directory. This requires no explicit configuration, but comes with the usual caveats about keeping important state on your local machine. Not suitable for production Local state is sufficient for a demonstration or evaluation, but for a shared or production deployment it is recommended to use remote state.","title":"Local state"},{"location":"repository/terraform/#remote-state","text":"Terraform supports a number of remote backends that can be used to persist Terraform state independently of where a deployment is run. This allows deployments to be made from anywhere that can access the state without corrupting or conflicting with any existing resources from previous deployments. Tip If you want to use the same remote backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. Warning In order to avoid multiple writers when using remote state, it is recommended to use a backend that supports state locking . Secret configuration Some of the configuration variables for remote backends, e.g. passwords and keys, should be kept secret. If you want to keep such variables in Git - which is recommended where possible - then they must be encrypted .","title":"Remote state"},{"location":"repository/terraform/#http","text":"The HTTP backend is a simple REST client that uses GET , POST and DELETE HTTP requests to manage a Terraform state. State locking is optional, but it is recommended to use an implementation that supports it. To use the HTTP backend, you must set at least the following variables: environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : http # The state endpoint for the environment # # Using the azimuth_environment variable in the address means that each # concrete environment gets different state URL even if this configuration # is in a shared mixin environment terraform_backend_config : address : \"https://example.org/tfstate/{{ azimuth_environment }}\" For the full set of available config options, see the Terraform docs for the HTTP backend .","title":"HTTP"},{"location":"repository/terraform/#gitlab","text":"Tip This is the recommended option if you are using GitLab for your config repository. If you are using GitLab to host your configuration repository, either gitlab.com or self-hosted, you can use GitLab-managed Terraform state to store the Terraform state for your environments. GitLab provides a HTTP backend that can be configured as follows: environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : http # The API base URL for the target GitLab project # For a self-hosted GitLab instance, replace gitlab.com with your domain gitlab_project_url : \"https://gitlab.com/api/v4/projects/<project id>\" # The state endpoint for the environment # # Using the azimuth_environment variable as the state name means that each # concrete environment gets a separate managed Terraform state even if this # configuration is in a shared mixin environment terraform_http_address : \"{{ gitlab_project_url }}/terraform/state/{{ azimuth_environment }}\" # The state-locking and unlocking endpoints for the environment terraform_http_lock_address : \"{{ terraform_http_address }}/lock\" terraform_http_lock_method : POST terraform_http_unlock_address : \"{{ terraform_http_lock_address }}\" terraform_http_unlock_method : DELETE terraform_backend_config : address : \"{{ terraform_http_address }}\" lock_address : \"{{ terraform_http_lock_address }}\" lock_method : \"{{ terraform_http_lock_method }}\" unlock_address : \"{{ terraform_http_unlock_address }}\" unlock_method : \"{{ terraform_http_unlock_method }}\" Tip If you want to use the same backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. The username and password (or token) that are used to authenticate with GitLab to manage the Terraform state are set using the TF_HTTP_USERNAME and TF_HTTP_PASSWORD environment variables respectively. If you are using GitLab CI/CD to automate deployments , then the pipeline will be issued with a suitable token. The sample configuration includes configuration to populate these variables using this token. If you are not using automation but your GitLab installation has project access tokens available, you can configure a project access token and store it (encrypted!) in the env.secret file, referencing the bot username: env.secret TF_HTTP_USERNAME = \"project_<id>_bot\" TF_HTTP_PASSWORD = \"<project access token>\" If you need to access an environment deployed using automation, or you do not have project access tokens available, then you can use a Personal access token , which at least avoids using your password. Never commit personal access tokens You should never commit a personal access token to the configuration repository, even encrypted, because it is not possible to set a project scope. If using a personal access token, you should export the relevant variables before activating an environment: export TF_HTTP_USERNAME = \"<username>\" export TF_HTTP_PASSWORD = \"<token>\" source ./bin/activate my-site","title":"GitLab"},{"location":"repository/terraform/#s3","text":"Terraform also has an S3 backend that is able to store state in any S3-compatible object store, such as Amazon S3 or Ceph Object Gateway . Depending on the provider of your object store, the specific configuration options in the following section may differ. The example configuration shown here is for a Ceph object store as Ceph is often used together with OpenStack. See the Terraform docs for the S3 backend for all the available options. environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : s3 # The endpoint of the object store terraform_s3_endpoint : object.example.com # The region to use # Ceph does not normally use the region, but Terraform requires it terraform_s3_region : not-used-but-required terraform_s3_skip_region_validation : \"true\" # The bucket to put Terraform states in # NOTE: This bucket must already exist - it will not be created by Terraform terraform_s3_bucket : azimuth-terraform-states # The key to use for the state for the environment # # Using the azimuth_environment variable in the key means that the state # for each concrete environment is stored in a separate key, even if this # configuration is in a shared mixin environment terraform_s3_key : \"{{ azimuth_environment }}.tfstate\" # The STS API doesn't exist for Ceph terraform_s3_skip_credentials_validation : \"true\" # Tell Terraform to use path-style URLs, e.g. <host>/<bucket>, instead of # subdomain-style URLs, e.g. <bucket>.<host> terraform_s3_force_path_style : \"true\" terraform_backend_config : endpoint : \"{{ terraform_s3_endpoint }}\" region : \"{{ terraform_s3_region }}\" bucket : \"{{ terraform_s3_bucket }}\" key : \"{{ terraform_s3_key }}\" skip_credentials_validation : \"{{ terraform_s3_skip_credentials_validation }}\" force_path_style : \"{{ terraform_s3_force_path_style }}\" skip_region_validation : \"{{ terraform_s3_skip_region_validation }}\" The S3 credentials (access key ID and secret), can be specified either as environment variables or as Ansible variables. To use environment variables, just place the credentials in env.secret : env.secret AWS_ACCESS_KEY_ID = \"<access key id>\" AWS_SECRET_ACCESS_KEY = \"<secret key>\" To use Ansible variables, they should be added to secrets.yml and referenced in the terraform_backend_config variable: environments/my-site/inventory/group_vars/all/secrets.yml terraform_s3_access_key : \"<access key id>\" terraform_s3_secret_key : \"<secret key>\" environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_config : # ... other options ... access_key : \"{{ terraform_s3_access_key }}\" secret_key : \"{{ terraform_s3_secret_key }}\"","title":"S3"}]}